{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T00:59:37.950209Z",
     "start_time": "2019-04-02T00:59:36.776042Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "import os\n",
    "import hickle as hkl\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn import preprocessing\n",
    "import heapq\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T00:59:38.933587Z",
     "start_time": "2019-04-02T00:59:38.928434Z"
    }
   },
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2): \n",
    "    lst3 = [value for value in lst1 if value in lst2] \n",
    "    return lst3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#darmanis\n",
    "romanov_feature = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/romanov/romanov_feature.csv').iloc[:,1].values.tolist()\n",
    "zeisel_feature = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/zeisel/zeisel_feature.csv').iloc[:,1].values.tolist()\n",
    "darmanis_feature = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/darmanis/darmanis_feature.csv').iloc[:,1].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "romanov_feature = [i.upper() for  i in romanov_feature]\n",
    "zeisel_feature = [i.upper() for  i in zeisel_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "romanov_zeisel = intersection(romanov_feature, zeisel_feature)\n",
    "romanov_zeisel_darmanis =  intersection(romanov_zeisel,darmanis_feature)\n",
    "romanov_feature_share = [romanov_feature.index(i) for i in romanov_zeisel_darmanis]\n",
    "zeisel_feature_share = [zeisel_feature.index(i) for i in romanov_zeisel_darmanis]\n",
    "darmanis_feature_share = [darmanis_feature.index(i) for i in romanov_zeisel_darmanis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24341"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(romanov_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24341, 2881)\n",
      "(19972, 3005)\n",
      "(22088, 285)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'segerstolpe'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(romanov_logcounts.shape)\n",
    "print(zeisel_logcounts.shape)\n",
    "print(darmanis_logcounts.shape)\n",
    "test_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "romanov_logcounts = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/romanov/romanov_logcounts.csv').iloc[:,1:].values\n",
    "zeisel_logcounts = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/zeisel/zeisel_logcounts.csv').iloc[:,1:].values\n",
    "darmanis_logcounts = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/darmanis/darmanis_logcounts.csv').iloc[:,1:].values\n",
    "romanov_logcounts_share = romanov_logcounts[romanov_feature_share, :]\n",
    "zeisel_logcounts_share = zeisel_logcounts[zeisel_feature_share, :]\n",
    "darmanis_logcounts_share = darmanis_logcounts[darmanis_feature_share, :]\n",
    "feature_number = len(romanov_zeisel_darmanis)\n",
    "romanov_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/romanov/romanov_label.csv').iloc[:,1].values.tolist()\n",
    "zeisel_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/zeisel/zeisel_label.csv').iloc[:,1].values.tolist()\n",
    "darmanis_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/darmanis/darmanis_label.csv').iloc[:,1].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(romanov_label)):\n",
    "    if romanov_label[i] == 'oligos':\n",
    "        romanov_label[i] = 'oligodendrocytes'\n",
    "for i in range(len(zeisel_label)):\n",
    "    if zeisel_label[i] == 'ca1pyramidal':\n",
    "        zeisel_label[i] = 'neurons'\n",
    "    if zeisel_label[i] == 's1pyramidal':\n",
    "        zeisel_label[i] = 'neurons'\n",
    "    if zeisel_label[i] == 'interneurons':\n",
    "        zeisel_label[i] = 'neurons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "oligodendrocytes    1001\n",
       "neurons              898\n",
       "ependymal            356\n",
       "astrocytes           267\n",
       "endothelial          240\n",
       "vsm                   71\n",
       "microglia             48\n",
       "dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(romanov_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neurons             131\n",
       "astrocytes           62\n",
       "oligodendrocytes     38\n",
       "endothelial          20\n",
       "OPC                  18\n",
       "microglia            16\n",
       "dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(darmanis_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = \"darmanis\"\n",
    "data_train = np.hstack((romanov_logcounts_share, zeisel_logcounts_share))\n",
    "test_logcounts_share = darmanis_logcounts_share\n",
    "label_test = darmanis_label\n",
    "label_train = romanov_label+zeisel_label\n",
    "if not os.path.exists('/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual'%test_name):\n",
    "    os.makedirs('/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual'%test_name)\n",
    "hkl.dump(label_train, '/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual/data_train_label.hkl'%test_name)\n",
    "hkl.dump(label_test, '/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual/data_test_label.hkl'%test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "data = np.hstack((romanov_logcounts_share,zeisel_logcounts_share,darmanis_logcounts_share))\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_ = min_max_scaler.fit_transform(data.T)\n",
    "X_train = min_max_scaler.transform(data_train.T)\n",
    "X_test = min_max_scaler.transform(test_logcounts_share.T)\n",
    "pca=PCA(n_components=100)\n",
    "pca.fit(X_)\n",
    "PCA_X_train = pca.transform(X_train)\n",
    "PCA_X_test = pca.transform(X_test)\n",
    "hkl.dump(PCA_X_train, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_PCA.hkl'%(test_name,100))\n",
    "hkl.dump(PCA_X_test, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_PCA.hkl'%(test_name,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 460147 论文方法\n",
    "data_original = np.exp(romanov_logcounts_share) - 1\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "data_train_var = np.var(data_original,1)\n",
    "data_train_mean_log = np.log(data_train_mean+1)\n",
    "data_train_var_log = np.log(data_train_var+1)\n",
    "ploy_reg = PolynomialFeatures(degree=2)\n",
    "x_ = ploy_reg.fit_transform(data_train_mean_log.reshape(-1,1))\n",
    "regr2 = linear_model.LinearRegression()\n",
    "regr2.fit(x_,data_train_var_log)\n",
    "y_pred = regr2.predict(x_)\n",
    "romanov_scores = (data_train_var_log-y_pred).reshape(-1)\n",
    "data_original = np.exp(zeisel_logcounts_share) - 1\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "data_train_var = np.var(data_original,1)\n",
    "data_train_mean_log = np.log(data_train_mean+1)\n",
    "data_train_var_log = np.log(data_train_var+1)\n",
    "ploy_reg = PolynomialFeatures(degree=2)\n",
    "x_ = ploy_reg.fit_transform(data_train_mean_log.reshape(-1,1))\n",
    "regr2 = linear_model.LinearRegression()\n",
    "regr2.fit(x_,data_train_var_log)\n",
    "y_pred = regr2.predict(x_)\n",
    "zeisel_scores = (data_train_var_log-y_pred).reshape(-1)\n",
    "data_original = np.exp(darmanis_logcounts_share) - 1\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "data_train_var = np.var(data_original,1)\n",
    "data_train_mean_log = np.log(data_train_mean+1)\n",
    "data_train_var_log = np.log(data_train_var+1)\n",
    "ploy_reg = PolynomialFeatures(degree=2)\n",
    "x_ = ploy_reg.fit_transform(data_train_mean_log.reshape(-1,1))\n",
    "regr2 = linear_model.LinearRegression()\n",
    "regr2.fit(x_,data_train_var_log)\n",
    "y_pred = regr2.predict(x_)\n",
    "darmanis_scores = (data_train_var_log-y_pred).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "K = 1300\n",
    "ind_romanov = np.argpartition(romanov_scores, -K)[-K:].tolist()\n",
    "ind_zeisel = np.argpartition(zeisel_scores, -K)[-K:].tolist()\n",
    "ind_darmanis = np.argpartition(darmanis_scores, -K)[-K:].tolist()\n",
    "ind_romanov_zeisel = intersection(ind_romanov, ind_zeisel)\n",
    "ind_romanov_zeisel_darmanis = intersection(ind_romanov_zeisel, ind_darmanis)\n",
    "print(len(ind_romanov_zeisel_darmanis))\n",
    "filtered_data_train = data_train[ind_romanov_zeisel_darmanis, :]\n",
    "filtered_data_test = test_logcounts_share[ind_romanov_zeisel_darmanis, :]\n",
    "K = 100\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_460147.hkl'%(test_name,K))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_460147.hkl'%(test_name,K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myself 自己方法\n",
    "zeisel_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(zeisel_logcounts_share) - 1\n",
    "drop_feature = []\n",
    "X1 = []\n",
    "Y = []\n",
    "X2 = []\n",
    "feature_index_dict = []\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        Y.append(np.log(data_train_mean[i]+1))\n",
    "        X1.append(np.mean(zeisel_logcounts_share[i,:]))\n",
    "        X2.append(np.log(drop_feature[i]))\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "X1 = np.array(X1).reshape(-1,1)\n",
    "X2 = np.array(X2).reshape(-1,1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X2,Y)\n",
    "predictions = lr.predict(X2)\n",
    "residuals = (Y-predictions).reshape(-1) + (Y - X1).reshape(-1)\n",
    "zeisel_scores[feature_index_dict] = residuals\n",
    "romanov_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(romanov_logcounts_share) - 1\n",
    "drop_feature = []\n",
    "X1 = []\n",
    "Y = []\n",
    "X2 = []\n",
    "feature_index_dict = []\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        Y.append(np.log(data_train_mean[i]+1))\n",
    "        X1.append(np.mean(romanov_logcounts_share[i,:]))\n",
    "        X2.append(np.log(drop_feature[i]))\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "X1 = np.array(X1).reshape(-1,1)\n",
    "X2 = np.array(X2).reshape(-1,1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X2,Y)\n",
    "predictions = lr.predict(X2)\n",
    "residuals = (Y-predictions).reshape(-1) + (Y - X1).reshape(-1)\n",
    "romanov_scores[feature_index_dict] = residuals\n",
    "darmanis_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(darmanis_logcounts_share) - 1\n",
    "drop_feature = []\n",
    "X1 = []\n",
    "Y = []\n",
    "X2 = []\n",
    "feature_index_dict = []\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        Y.append(np.log(data_train_mean[i]+1))\n",
    "        X1.append(np.mean(darmanis_logcounts_share[i,:]))\n",
    "        X2.append(np.log(drop_feature[i]))\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "X1 = np.array(X1).reshape(-1,1)\n",
    "X2 = np.array(X2).reshape(-1,1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X2,Y)\n",
    "predictions = lr.predict(X2)\n",
    "residuals = (Y-predictions).reshape(-1) + (Y - X1).reshape(-1)\n",
    "darmanis_scores[feature_index_dict] = residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "K = 641\n",
    "ind_romanov = np.argpartition(romanov_scores, -K)[-K:].tolist()\n",
    "ind_zeisel = np.argpartition(zeisel_scores, -K)[-K:].tolist()\n",
    "ind_darmanis = np.argpartition(darmanis_scores, -K)[-K:].tolist()\n",
    "ind_romanov_zeisel = intersection(ind_romanov, ind_zeisel)\n",
    "ind_romanov_zeisel_darmanis = intersection(ind_romanov_zeisel, ind_darmanis)\n",
    "print(len(ind_romanov_zeisel_darmanis))\n",
    "filtered_data_train = data_train[ind_romanov_zeisel_darmanis, :]\n",
    "filtered_data_test = test_logcounts_share[ind_romanov_zeisel_darmanis, :]\n",
    "K = 100\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_myself.hkl'%(test_name,K))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_myself.hkl'%(test_name,K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14492,)\n",
      "(14651,)\n",
      "(14487,)\n"
     ]
    }
   ],
   "source": [
    "#scmap\n",
    "romanov_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(romanov_logcounts_share) - 1\n",
    "data_original_mean = np.mean(data_original,1)\n",
    "drop_feature = []\n",
    "\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "#print(drop_feature)\n",
    "feature_index_dict = []\n",
    "train_E = []\n",
    "train_D = []\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        train_E.append(data_original_mean[i])\n",
    "        train_D.append(drop_feature[i])\n",
    "train_E = np.array(train_E)\n",
    "train_D = np.array(train_D)\n",
    "train_E = np.log(train_E+1).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "train_D = np.log(100*train_D).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_E,train_D)\n",
    "predictions = lr.predict(train_E)\n",
    "residuals = (train_D-predictions).reshape(-1)\n",
    "#residuals = np.abs(residuals)\n",
    "romanov_scores[feature_index_dict] = residuals\n",
    "print(residuals.shape)\n",
    "\n",
    "zeisel_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(zeisel_logcounts_share) - 1\n",
    "data_original_mean = np.mean(data_original,1)\n",
    "drop_feature = []\n",
    "\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "#print(drop_feature)\n",
    "feature_index_dict = []\n",
    "train_E = []\n",
    "train_D = []\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        train_E.append(data_original_mean[i])\n",
    "        train_D.append(drop_feature[i])\n",
    "train_E = np.array(train_E)\n",
    "train_D = np.array(train_D)\n",
    "train_E = np.log(train_E+1).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "train_D = np.log(100*train_D).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_E,train_D)\n",
    "predictions = lr.predict(train_E)\n",
    "residuals = (train_D-predictions).reshape(-1)\n",
    "#residuals = np.abs(residuals)\n",
    "zeisel_scores[feature_index_dict] = residuals\n",
    "print(residuals.shape)\n",
    "\n",
    "darmanis_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(darmanis_logcounts_share) - 1\n",
    "data_original_mean = np.mean(data_original,1)\n",
    "drop_feature = []\n",
    "\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "#print(drop_feature)\n",
    "feature_index_dict = []\n",
    "train_E = []\n",
    "train_D = []\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        train_E.append(data_original_mean[i])\n",
    "        train_D.append(drop_feature[i])\n",
    "train_E = np.array(train_E)\n",
    "train_D = np.array(train_D)\n",
    "train_E = np.log(train_E+1).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "train_D = np.log(100*train_D).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_E,train_D)\n",
    "predictions = lr.predict(train_E)\n",
    "residuals = (train_D-predictions).reshape(-1)\n",
    "#residuals = np.abs(residuals)\n",
    "darmanis_scores[feature_index_dict] = residuals\n",
    "print(residuals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "K = 745\n",
    "ind_romanov = np.argpartition(romanov_scores, -K)[-K:].tolist()\n",
    "ind_zeisel = np.argpartition(zeisel_scores, -K)[-K:].tolist()\n",
    "ind_darmanis = np.argpartition(darmanis_scores, -K)[-K:].tolist()\n",
    "ind_romanov_zeisel = intersection(ind_romanov, ind_zeisel)\n",
    "ind_romanov_zeisel_darmanis = intersection(ind_romanov_zeisel, ind_darmanis)\n",
    "print(len(ind_romanov_zeisel_darmanis))\n",
    "filtered_data_train = data_train[ind_romanov_zeisel_darmanis, :]\n",
    "filtered_data_test = test_logcounts_share[ind_romanov_zeisel_darmanis, :]\n",
    "K = 100\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_scmap.hkl'%(test_name,K))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_scmap.hkl'%(test_name,K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14651, 5886)\n"
     ]
    }
   ],
   "source": [
    "#superCT\n",
    "print(data_train.shape)\n",
    "filtered_data_train = np.int64(data_train>0)\n",
    "filtered_data_test = np.int64(test_logcounts_share>0)\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_SuperCT.hkl'%(test_name,100))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_SuperCT.hkl'%(test_name,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#romanov zeisel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "romanov_feature = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/romanov/romanov_feature.csv').iloc[:,1].values.tolist()\n",
    "zeisel_feature = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/zeisel/zeisel_feature.csv').iloc[:,1].values.tolist()\n",
    "romanov_zeisel = intersection(romanov_feature, zeisel_feature)\n",
    "romanov_feature_share = [romanov_feature.index(i) for i in romanov_zeisel]\n",
    "zeisel_feature_share = [zeisel_feature.index(i) for i in romanov_zeisel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "romanov_logcounts = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/romanov/romanov_logcounts.csv').iloc[:,1:].values\n",
    "zeisel_logcounts = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/zeisel/zeisel_logcounts.csv').iloc[:,1:].values\n",
    "romanov_logcounts_share = romanov_logcounts[romanov_feature_share, :]\n",
    "zeisel_logcounts_share = zeisel_logcounts[zeisel_feature_share, :]\n",
    "feature_number = len(romanov_zeisel)\n",
    "romanov_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/romanov/romanov_label.csv').iloc[:,1].values.tolist()\n",
    "zeisel_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/zeisel/zeisel_label.csv').iloc[:,1].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "oligodendrocytes    1001\n",
       "neurons              898\n",
       "ependymal            356\n",
       "astrocytes           267\n",
       "endothelial          240\n",
       "vsm                   71\n",
       "microglia             48\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(romanov_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(romanov_label)):\n",
    "    if romanov_label[i] == 'oligos':\n",
    "        romanov_label[i] = 'oligodendrocytes'\n",
    "for i in range(len(zeisel_label)):\n",
    "    if zeisel_label[i] == 'ca1pyramidal':\n",
    "        zeisel_label[i] = 'neurons'\n",
    "    if zeisel_label[i] == 's1pyramidal':\n",
    "        zeisel_label[i] = 'neurons'\n",
    "    if zeisel_label[i] == 'interneurons':\n",
    "        zeisel_label[i] = 'neurons'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neurons             1628\n",
       "oligodendrocytes     820\n",
       "astrocytes           198\n",
       "endothelial          175\n",
       "microglia             98\n",
       "mural                 60\n",
       "ependymal             26\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(zeisel_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = 'zeisel'\n",
    "if test_name == 'zeisel':\n",
    "    data_train = romanov_logcounts_share\n",
    "    test_logcounts_share = zeisel_logcounts_share\n",
    "    label_test = zeisel_label\n",
    "    label_train = romanov_label\n",
    "if test_name == 'romanov':\n",
    "    data_train = zeisel_logcounts_share\n",
    "    test_logcounts_share = romanov_logcounts_share\n",
    "    label_test = romanov_label\n",
    "    label_train = zeisel_label\n",
    "if not os.path.exists('/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual'%test_name):\n",
    "    os.makedirs('/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual'%test_name)\n",
    "hkl.dump(label_train, '/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual/data_train_label.hkl'%test_name)\n",
    "hkl.dump(label_test, '/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual/data_test_label.hkl'%test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19376,)\n",
      "(19971,)\n"
     ]
    }
   ],
   "source": [
    "#scmap\n",
    "romanov_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(romanov_logcounts_share) - 1\n",
    "data_original_mean = np.mean(data_original,1)\n",
    "drop_feature = []\n",
    "\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "#print(drop_feature)\n",
    "feature_index_dict = []\n",
    "train_E = []\n",
    "train_D = []\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        train_E.append(data_original_mean[i])\n",
    "        train_D.append(drop_feature[i])\n",
    "train_E = np.array(train_E)\n",
    "train_D = np.array(train_D)\n",
    "train_E = np.log(train_E+1).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "train_D = np.log(100*train_D).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_E,train_D)\n",
    "predictions = lr.predict(train_E)\n",
    "residuals = (train_D-predictions).reshape(-1)\n",
    "#residuals = np.abs(residuals)\n",
    "romanov_scores[feature_index_dict] = residuals\n",
    "print(residuals.shape)\n",
    "\n",
    "zeisel_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(zeisel_logcounts_share) - 1\n",
    "data_original_mean = np.mean(data_original,1)\n",
    "drop_feature = []\n",
    "\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "#print(drop_feature)\n",
    "feature_index_dict = []\n",
    "train_E = []\n",
    "train_D = []\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        train_E.append(data_original_mean[i])\n",
    "        train_D.append(drop_feature[i])\n",
    "train_E = np.array(train_E)\n",
    "train_D = np.array(train_D)\n",
    "train_E = np.log(train_E+1).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "train_D = np.log(100*train_D).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_E,train_D)\n",
    "predictions = lr.predict(train_E)\n",
    "residuals = (train_D-predictions).reshape(-1)\n",
    "#residuals = np.abs(residuals)\n",
    "zeisel_scores[feature_index_dict] = residuals\n",
    "print(residuals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "K = 149\n",
    "ind_romanov = np.argpartition(romanov_scores, -K)[-K:].tolist()\n",
    "ind_zeisel = np.argpartition(zeisel_scores, -K)[-K:].tolist()\n",
    "ind_romanov_zeisel = intersection(ind_romanov, ind_zeisel)\n",
    "print(len(ind_romanov_zeisel))\n",
    "filtered_data_train = data_train[ind_romanov_zeisel, :]\n",
    "filtered_data_test = test_logcounts_share[ind_romanov_zeisel, :]\n",
    "K = 100\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_scmap.hkl'%(test_name,K))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_scmap.hkl'%(test_name,K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19971, 3005)\n"
     ]
    }
   ],
   "source": [
    "#superCT\n",
    "print(data_train.shape)\n",
    "filtered_data_train = np.int64(data_train>0)\n",
    "filtered_data_test = np.int64(test_logcounts_share>0)\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_SuperCT.hkl'%(test_name,100))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_SuperCT.hkl'%(test_name,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 460147 论文方法\n",
    "data_original = np.exp(romanov_logcounts_share) - 1\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "data_train_var = np.var(data_original,1)\n",
    "data_train_mean_log = np.log(data_train_mean+1)\n",
    "data_train_var_log = np.log(data_train_var+1)\n",
    "ploy_reg = PolynomialFeatures(degree=2)\n",
    "x_ = ploy_reg.fit_transform(data_train_mean_log.reshape(-1,1))\n",
    "regr2 = linear_model.LinearRegression()\n",
    "regr2.fit(x_,data_train_var_log)\n",
    "y_pred = regr2.predict(x_)\n",
    "romanov_scores = (data_train_var_log-y_pred).reshape(-1)\n",
    "data_original = np.exp(zeisel_logcounts_share) - 1\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "data_train_var = np.var(data_original,1)\n",
    "data_train_mean_log = np.log(data_train_mean+1)\n",
    "data_train_var_log = np.log(data_train_var+1)\n",
    "ploy_reg = PolynomialFeatures(degree=2)\n",
    "x_ = ploy_reg.fit_transform(data_train_mean_log.reshape(-1,1))\n",
    "regr2 = linear_model.LinearRegression()\n",
    "regr2.fit(x_,data_train_var_log)\n",
    "y_pred = regr2.predict(x_)\n",
    "zeisel_scores = (data_train_var_log-y_pred).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "K = 210\n",
    "ind_romanov = np.argpartition(romanov_scores, -K)[-K:].tolist()\n",
    "ind_zeisel = np.argpartition(zeisel_scores, -K)[-K:].tolist()\n",
    "ind_romanov_zeisel = intersection(ind_romanov, ind_zeisel)\n",
    "print(len(ind_romanov_zeisel))\n",
    "filtered_data_train = data_train[ind_romanov_zeisel, :]\n",
    "filtered_data_test = test_logcounts_share[ind_romanov_zeisel, :]\n",
    "K = 100\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_460147.hkl'%(test_name,K))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_460147.hkl'%(test_name,K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myself 自己方法\n",
    "zeisel_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(zeisel_logcounts_share) - 1\n",
    "drop_feature = []\n",
    "X1 = []\n",
    "Y = []\n",
    "X2 = []\n",
    "feature_index_dict = []\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        Y.append(np.log(data_train_mean[i]+1))\n",
    "        X1.append(np.mean(zeisel_logcounts_share[i,:]))\n",
    "        X2.append(np.log(drop_feature[i]))\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "X1 = np.array(X1).reshape(-1,1)\n",
    "X2 = np.array(X2).reshape(-1,1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X2,Y)\n",
    "predictions = lr.predict(X2)\n",
    "residuals = (Y-predictions).reshape(-1) + (Y - X1).reshape(-1)\n",
    "zeisel_scores[feature_index_dict] = residuals\n",
    "romanov_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(romanov_logcounts_share) - 1\n",
    "drop_feature = []\n",
    "X1 = []\n",
    "Y = []\n",
    "X2 = []\n",
    "feature_index_dict = []\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        Y.append(np.log(data_train_mean[i]+1))\n",
    "        X1.append(np.mean(romanov_logcounts_share[i,:]))\n",
    "        X2.append(np.log(drop_feature[i]))\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "X1 = np.array(X1).reshape(-1,1)\n",
    "X2 = np.array(X2).reshape(-1,1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X2,Y)\n",
    "predictions = lr.predict(X2)\n",
    "residuals = (Y-predictions).reshape(-1) + (Y - X1).reshape(-1)\n",
    "romanov_scores[feature_index_dict] = residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "K = 156\n",
    "ind_romanov = np.argpartition(romanov_scores, -K)[-K:].tolist()\n",
    "ind_zeisel = np.argpartition(zeisel_scores, -K)[-K:].tolist()\n",
    "ind_romanov_zeisel = intersection(ind_romanov, ind_zeisel)\n",
    "print(len(ind_romanov_zeisel))\n",
    "filtered_data_train = data_train[ind_romanov_zeisel, :]\n",
    "filtered_data_test = test_logcounts_share[ind_romanov_zeisel, :]\n",
    "K = 100\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_myself.hkl'%(test_name,K))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_myself.hkl'%(test_name,K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "data = np.hstack((romanov_logcounts_share,zeisel_logcounts_share))\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_ = min_max_scaler.fit_transform(data.T)\n",
    "X_train = min_max_scaler.transform(data_train.T)\n",
    "X_test = min_max_scaler.transform(test_logcounts_share.T)\n",
    "pca=PCA(n_components=100)\n",
    "pca.fit(X_)\n",
    "PCA_X_train = pca.transform(X_train)\n",
    "PCA_X_test = pca.transform(X_test)\n",
    "hkl.dump(PCA_X_train, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_PCA.hkl'%(test_name,100))\n",
    "hkl.dump(PCA_X_test, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_PCA.hkl'%(test_name,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#小鼠视网膜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "shekhar_feature = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/shekhar/shekhar_feature.csv').iloc[:,1].values.tolist()\n",
    "macosko_feature = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/macosko/macosko_feature.csv').iloc[:,1].values.tolist()\n",
    "shekhar_macosko = intersection(shekhar_feature, macosko_feature)\n",
    "shekhar_feature_share = [shekhar_feature.index(i) for i in shekhar_macosko]\n",
    "macosko_feature_share = [macosko_feature.index(i) for i in shekhar_macosko]\n",
    "shekhar_logcounts = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/shekhar/shekhar_logcounts.csv').iloc[:,1:].values\n",
    "macosko_logcounts = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/macosko/macosko_logcounts.csv').iloc[:,1:].values\n",
    "shekhar_logcounts_share = shekhar_logcounts[shekhar_feature_share, :]\n",
    "macosko_logcounts_share = macosko_logcounts[macosko_feature_share, :]\n",
    "feature_number = len(shekhar_macosko)\n",
    "shekhar_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/shekhar/shekhar_label.csv').iloc[:,1].values.tolist()\n",
    "macosko_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/macosko/macosko_label.csv').iloc[:,1].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rods                    29400\n",
       "bipolar                  6285\n",
       "amacrine                 4426\n",
       "cones                    1868\n",
       "muller                   1624\n",
       "ganglion                  432\n",
       "horizontal                252\n",
       "vascular_endothelium      252\n",
       "fibroblasts                85\n",
       "microglia                  67\n",
       "pericytes                  63\n",
       "astrocytes                 54\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(macosko_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bipolar     23494\n",
       "muller       2945\n",
       "amacrine      252\n",
       "rods           91\n",
       "cones          48\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(shekhar_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = 'macosko'\n",
    "if test_name == 'macosko':\n",
    "    data_train = shekhar_logcounts_share\n",
    "    test_logcounts_share = macosko_logcounts_share\n",
    "    label_test = macosko_label\n",
    "    label_train = shekhar_label\n",
    "if test_name == 'shekhar':\n",
    "    data_train = macosko_logcounts_share\n",
    "    test_logcounts_share = shekhar_logcounts_share\n",
    "    label_test = shekhar_label\n",
    "    label_train = macosko_label\n",
    "if not os.path.exists('/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual'%test_name):\n",
    "    os.makedirs('/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual'%test_name)\n",
    "hkl.dump(label_train, '/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual/data_train_label.hkl'%test_name)\n",
    "hkl.dump(label_test, '/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual/data_test_label.hkl'%test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12333,)\n",
      "(12332,)\n"
     ]
    }
   ],
   "source": [
    "#scmap\n",
    "shekhar_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(shekhar_logcounts_share) - 1\n",
    "data_original_mean = np.mean(data_original,1)\n",
    "drop_feature = []\n",
    "\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "#print(drop_feature)\n",
    "feature_index_dict = []\n",
    "train_E = []\n",
    "train_D = []\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        train_E.append(data_original_mean[i])\n",
    "        train_D.append(drop_feature[i])\n",
    "train_E = np.array(train_E)\n",
    "train_D = np.array(train_D)\n",
    "train_E = np.log(train_E+1).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "train_D = np.log(100*train_D).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_E,train_D)\n",
    "predictions = lr.predict(train_E)\n",
    "residuals = (train_D-predictions).reshape(-1)\n",
    "#residuals = np.abs(residuals)\n",
    "shekhar_scores[feature_index_dict] = residuals\n",
    "print(residuals.shape)\n",
    "\n",
    "macosko_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(macosko_logcounts_share) - 1\n",
    "data_original_mean = np.mean(data_original,1)\n",
    "drop_feature = []\n",
    "\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "#print(drop_feature)\n",
    "feature_index_dict = []\n",
    "train_E = []\n",
    "train_D = []\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        train_E.append(data_original_mean[i])\n",
    "        train_D.append(drop_feature[i])\n",
    "train_E = np.array(train_E)\n",
    "train_D = np.array(train_D)\n",
    "train_E = np.log(train_E+1).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "train_D = np.log(100*train_D).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_E,train_D)\n",
    "predictions = lr.predict(train_E)\n",
    "residuals = (train_D-predictions).reshape(-1)\n",
    "#residuals = np.abs(residuals)\n",
    "macosko_scores[feature_index_dict] = residuals\n",
    "print(residuals.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "K = 298\n",
    "ind_shekhar = np.argpartition(shekhar_scores, -K)[-K:].tolist()\n",
    "ind_macosko = np.argpartition(macosko_scores, -K)[-K:].tolist()\n",
    "ind_shekhar_macosko = intersection(ind_shekhar, ind_macosko)\n",
    "print(len(ind_shekhar_macosko))\n",
    "filtered_data_train = data_train[ind_shekhar_macosko, :]\n",
    "filtered_data_test = test_logcounts_share[ind_shekhar_macosko, :]\n",
    "K = 100\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_scmap.hkl'%(test_name,K))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_scmap.hkl'%(test_name,K))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12333, 26830)\n"
     ]
    }
   ],
   "source": [
    "#superCT\n",
    "print(data_train.shape)\n",
    "filtered_data_train = np.int64(data_train>0)\n",
    "filtered_data_test = np.int64(test_logcounts_share>0)\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_SuperCT.hkl'%(test_name,100))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_SuperCT.hkl'%(test_name,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 460147 论文方法\n",
    "data_original = np.exp(shekhar_logcounts_share) - 1\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "data_train_var = np.var(data_original,1)\n",
    "data_train_mean_log = np.log(data_train_mean+1)\n",
    "data_train_var_log = np.log(data_train_var+1)\n",
    "ploy_reg = PolynomialFeatures(degree=2)\n",
    "x_ = ploy_reg.fit_transform(data_train_mean_log.reshape(-1,1))\n",
    "regr2 = linear_model.LinearRegression()\n",
    "regr2.fit(x_,data_train_var_log)\n",
    "y_pred = regr2.predict(x_)\n",
    "shekhar_scores = (data_train_var_log-y_pred).reshape(-1)\n",
    "data_original = np.exp(macosko_logcounts_share) - 1\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "data_train_var = np.var(data_original,1)\n",
    "data_train_mean_log = np.log(data_train_mean+1)\n",
    "data_train_var_log = np.log(data_train_var+1)\n",
    "ploy_reg = PolynomialFeatures(degree=2)\n",
    "x_ = ploy_reg.fit_transform(data_train_mean_log.reshape(-1,1))\n",
    "regr2 = linear_model.LinearRegression()\n",
    "regr2.fit(x_,data_train_var_log)\n",
    "y_pred = regr2.predict(x_)\n",
    "macosko_scores = (data_train_var_log-y_pred).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "K = 228\n",
    "ind_shekhar = np.argpartition(shekhar_scores, -K)[-K:].tolist()\n",
    "ind_macosko = np.argpartition(macosko_scores, -K)[-K:].tolist()\n",
    "ind_shekhar_macosko = intersection(ind_shekhar, ind_macosko)\n",
    "print(len(ind_shekhar_macosko))\n",
    "filtered_data_train = data_train[ind_shekhar_macosko, :]\n",
    "filtered_data_test = test_logcounts_share[ind_shekhar_macosko, :]\n",
    "K = 100\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_460147.hkl'%(test_name,K))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_460147.hkl'%(test_name,K))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myself 自己方法\n",
    "macosko_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(macosko_logcounts_share) - 1\n",
    "drop_feature = []\n",
    "X1 = []\n",
    "Y = []\n",
    "X2 = []\n",
    "feature_index_dict = []\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        Y.append(np.log(data_train_mean[i]+1))\n",
    "        X1.append(np.mean(macosko_logcounts_share[i,:]))\n",
    "        X2.append(np.log(drop_feature[i]))\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "X1 = np.array(X1).reshape(-1,1)\n",
    "X2 = np.array(X2).reshape(-1,1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X2,Y)\n",
    "predictions = lr.predict(X2)\n",
    "residuals = (Y-predictions).reshape(-1) + (Y - X1).reshape(-1)\n",
    "macosko_scores[feature_index_dict] = residuals\n",
    "shekhar_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(shekhar_logcounts_share) - 1\n",
    "drop_feature = []\n",
    "X1 = []\n",
    "Y = []\n",
    "X2 = []\n",
    "feature_index_dict = []\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        Y.append(np.log(data_train_mean[i]+1))\n",
    "        X1.append(np.mean(shekhar_logcounts_share[i,:]))\n",
    "        X2.append(np.log(drop_feature[i]))\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "X1 = np.array(X1).reshape(-1,1)\n",
    "X2 = np.array(X2).reshape(-1,1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X2,Y)\n",
    "predictions = lr.predict(X2)\n",
    "residuals = (Y-predictions).reshape(-1) + (Y - X1).reshape(-1)\n",
    "shekhar_scores[feature_index_dict] = residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "K = 270\n",
    "ind_shekhar = np.argpartition(shekhar_scores, -K)[-K:].tolist()\n",
    "ind_macosko = np.argpartition(macosko_scores, -K)[-K:].tolist()\n",
    "ind_shekhar_macosko = intersection(ind_shekhar, ind_macosko)\n",
    "print(len(ind_shekhar_macosko))\n",
    "filtered_data_train = data_train[ind_shekhar_macosko, :]\n",
    "filtered_data_test = test_logcounts_share[ind_shekhar_macosko, :]\n",
    "K = 100\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_myself.hkl'%(test_name,K))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_myself.hkl'%(test_name,K))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "data = np.hstack((shekhar_logcounts_share,macosko_logcounts_share))\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_ = min_max_scaler.fit_transform(data.T)\n",
    "X_train = min_max_scaler.transform(data_train.T)\n",
    "X_test = min_max_scaler.transform(test_logcounts_share.T)\n",
    "pca=PCA(n_components=100)\n",
    "pca.fit(X_)\n",
    "PCA_X_train = pca.transform(X_train)\n",
    "PCA_X_test = pca.transform(X_test)\n",
    "hkl.dump(PCA_X_train, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_PCA.hkl'%(test_name,100))\n",
    "hkl.dump(PCA_X_test, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_PCA.hkl'%(test_name,100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#人类胰腺\n",
    "muraro_feature = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/muraro/muraro_feature.csv').iloc[:,1].values.tolist()\n",
    "baron_feature = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/baron/baron_feature.csv').iloc[:,1].values.tolist()\n",
    "xin_feature = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/xin/xin_feature.csv').iloc[:,1].values.tolist()\n",
    "segerstolpe_feature = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/segerstolpe/segerstolpe_feature.csv').iloc[:,1].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#找到共同基因集合\n",
    "muraro_baron = intersection(muraro_feature, baron_feature)\n",
    "muraro_baron_xin = intersection(muraro_baron, xin_feature)\n",
    "muraro_baron_xin_segerstolpe = intersection(muraro_baron_xin, segerstolpe_feature)\n",
    "#找到这些相同基因位置\n",
    "muraro_feature_share = [muraro_feature.index(i) for i in muraro_baron_xin_segerstolpe]\n",
    "baron_feature_share = [baron_feature.index(i) for i in muraro_baron_xin_segerstolpe]\n",
    "xin_feature_share = [xin_feature.index(i) for i in muraro_baron_xin_segerstolpe]\n",
    "segerstolpe_feature_share = [segerstolpe_feature.index(i) for i in muraro_baron_xin_segerstolpe]\n",
    "#读取数据集\n",
    "muraro_logcounts = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/muraro/muraro_logcounts.csv').iloc[:,1:].values\n",
    "baron_logcounts = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/baron/baron_logcounts.csv').iloc[:,1:].values\n",
    "xin_logcounts = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/xin/xin_logcounts.csv').iloc[:,1:].values\n",
    "segerstolpe_logcounts = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/segerstolpe/segerstolpe_logcounts.csv').iloc[:,1:].values\n",
    "#所用数据集（未进行降维）\n",
    "muraro_logcounts_share = muraro_logcounts[muraro_feature_share, :]\n",
    "baron_logcounts_share = baron_logcounts[baron_feature_share, :]\n",
    "xin_logcounts_share = xin_logcounts[xin_feature_share, :]\n",
    "segerstolpe_logcounts_share = segerstolpe_logcounts[segerstolpe_feature_share, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "muraro_share = np.exp(muraro_logcounts_share)-1\n",
    "muraro_share = pd.DataFrame(muraro_share)\n",
    "muraro_share.to_csv('/home/chenxiaoyang/program/scmap/Data/processed/muraro_share.csv')\n",
    "xin_share = np.exp(xin_logcounts_share)-1\n",
    "xin_share = pd.DataFrame(xin_share)\n",
    "xin_share.to_csv('/home/chenxiaoyang/program/scmap/Data/processed/xin_share.csv')\n",
    "baron_share = np.exp(baron_logcounts_share)-1\n",
    "baron_share = pd.DataFrame(baron_share)\n",
    "baron_share.to_csv('/home/chenxiaoyang/program/scmap/Data/processed/baron_share.csv')\n",
    "segerstolpe_share = np.exp(segerstolpe_logcounts_share)-1\n",
    "segerstolpe_share = pd.DataFrame(segerstolpe_share)\n",
    "segerstolpe_share.to_csv('/home/chenxiaoyang/program/scmap/Data/processed/segerstolpe_share.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_number = len(muraro_baron_xin_segerstolpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16088,)\n",
      "(15917,)\n",
      "(16243,)\n"
     ]
    }
   ],
   "source": [
    "#muraro scmap\n",
    "muraro_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(muraro_logcounts_share) - 1\n",
    "data_original_mean = np.mean(data_original,1)\n",
    "drop_feature = []\n",
    "\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "#print(drop_feature)\n",
    "feature_index_dict = []\n",
    "train_E = []\n",
    "train_D = []\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        train_E.append(data_original_mean[i])\n",
    "        train_D.append(drop_feature[i])\n",
    "train_E = np.array(train_E)\n",
    "train_D = np.array(train_D)\n",
    "train_E = np.log(train_E+1).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "train_D = np.log(100*train_D).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_E,train_D)\n",
    "predictions = lr.predict(train_E)\n",
    "residuals = (train_D-predictions).reshape(-1)\n",
    "#residuals = np.abs(residuals)\n",
    "muraro_scores[feature_index_dict] = residuals\n",
    "#xin scmap\n",
    "xin_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(xin_logcounts_share) - 1\n",
    "data_original_mean = np.mean(data_original,1)\n",
    "drop_feature = []\n",
    "\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "#print(drop_feature)\n",
    "feature_index_dict = []\n",
    "train_E = []\n",
    "train_D = []\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        train_E.append(data_original_mean[i])\n",
    "        train_D.append(drop_feature[i])\n",
    "train_E = np.array(train_E)\n",
    "train_D = np.array(train_D)\n",
    "train_E = np.log(train_E+1).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "train_D = np.log(100*train_D).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_E,train_D)\n",
    "predictions = lr.predict(train_E)\n",
    "residuals = (train_D-predictions).reshape(-1)\n",
    "#residuals = np.abs(residuals)\n",
    "xin_scores[feature_index_dict] = residuals\n",
    "print(residuals.shape)\n",
    "#baron scmap\n",
    "baron_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(baron_logcounts_share) - 1\n",
    "data_original_mean = np.mean(data_original,1)\n",
    "drop_feature = []\n",
    "\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "#print(drop_feature)\n",
    "feature_index_dict = []\n",
    "train_E = []\n",
    "train_D = []\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        train_E.append(data_original_mean[i])\n",
    "        train_D.append(drop_feature[i])\n",
    "train_E = np.array(train_E)\n",
    "train_D = np.array(train_D)\n",
    "train_E = np.log(train_E+1).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "train_D = np.log(100*train_D).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_E,train_D)\n",
    "predictions = lr.predict(train_E)\n",
    "residuals = (train_D-predictions).reshape(-1)\n",
    "#residuals = np.abs(residuals)\n",
    "baron_scores[feature_index_dict] = residuals\n",
    "print(residuals.shape)\n",
    "#segerstolpe scmap\n",
    "segerstolpe_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(segerstolpe_logcounts_share) - 1\n",
    "data_original_mean = np.mean(data_original,1)\n",
    "drop_feature = []\n",
    "\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "#print(drop_feature)\n",
    "feature_index_dict = []\n",
    "train_E = []\n",
    "train_D = []\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        train_E.append(data_original_mean[i])\n",
    "        train_D.append(drop_feature[i])\n",
    "train_E = np.array(train_E)\n",
    "train_D = np.array(train_D)\n",
    "train_E = np.log(train_E+1).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "train_D = np.log(100*train_D).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_E,train_D)\n",
    "predictions = lr.predict(train_E)\n",
    "residuals = (train_D-predictions).reshape(-1)\n",
    "#residuals = np.abs(residuals)\n",
    "segerstolpe_scores[feature_index_dict] = residuals\n",
    "print(residuals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 1229\n",
    "ind_muraro = np.argpartition(muraro_scores, -K)[-K:].tolist()\n",
    "ind_baron = np.argpartition(baron_scores, -K)[-K:].tolist()\n",
    "ind_xin = np.argpartition(xin_scores, -K)[-K:].tolist()\n",
    "ind_segerstolpe = np.argpartition(segerstolpe_scores, -K)[-K:].tolist()\n",
    "\n",
    "ind_muraro_baron = intersection(ind_muraro, ind_baron)\n",
    "ind_muraro_baron_xin = intersection(ind_muraro_baron, ind_xin)\n",
    "ind_muraro_baron_xin_segerstolpe = intersection(ind_muraro_baron_xin, ind_segerstolpe)\n",
    "len(ind_muraro_baron_xin_segerstolpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ind_muraro_baron_xin_segerstolpe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2b07c9900b99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtest_logcounts_share\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegerstolpe_logcounts_share\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mhkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual/data_train_label.hkl'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mtest_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mfiltered_data_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind_muraro_baron_xin_segerstolpe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mfiltered_data_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_logcounts_share\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind_muraro_baron_xin_segerstolpe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ind_muraro_baron_xin_segerstolpe' is not defined"
     ]
    }
   ],
   "source": [
    "test_name = 'segerstolpe'\n",
    "if test_name == 'muraro':\n",
    "    data_train = np.hstack((baron_logcounts_share, xin_logcounts_share, segerstolpe_logcounts_share))\n",
    "if test_name == 'baron':\n",
    "    data_train = np.hstack((muraro_logcounts_share, xin_logcounts_share, segerstolpe_logcounts_share))\n",
    "if test_name == 'xin':\n",
    "    data_train = np.hstack((muraro_logcounts_share, baron_logcounts_share, segerstolpe_logcounts_share))\n",
    "if test_name == 'segerstolpe':\n",
    "    data_train = np.hstack((xin_logcounts_share, baron_logcounts_share, muraro_logcounts_share))\n",
    "muraro_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/muraro/muraro_label.csv').iloc[:,1].values.tolist()\n",
    "baron_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/baron/baron_label.csv').iloc[:,1].values.tolist()\n",
    "xin_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/xin/xin_label.csv').iloc[:,1].values.tolist()\n",
    "segerstolpe_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/segerstolpe/segerstolpe_label.csv').iloc[:,1].values.tolist()\n",
    "if test_name == 'muraro':\n",
    "    label_train = baron_label+xin_label+segerstolpe_label\n",
    "    test_logcounts_share = muraro_logcounts_share\n",
    "if test_name == 'baron':\n",
    "    label_train = muraro_label+xin_label+segerstolpe_label\n",
    "    test_logcounts_share = baron_logcounts_share\n",
    "if test_name == 'xin':\n",
    "    label_train = muraro_label+baron_label+segerstolpe_label\n",
    "    test_logcounts_share = xin_logcounts_share\n",
    "if test_name == 'segerstolpe':\n",
    "    label_train = xin_label+baron_label+muraro_label\n",
    "    test_logcounts_share = segerstolpe_logcounts_share\n",
    "hkl.dump(label_train, '/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual/data_train_label.hkl'%test_name)\n",
    "filtered_data_train = data_train[ind_muraro_baron_xin_segerstolpe, :]\n",
    "filtered_data_test = test_logcounts_share[ind_muraro_baron_xin_segerstolpe, :]\n",
    "K = 100\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_scmap.hkl'%(test_name,K))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_scmap.hkl'%(test_name,K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16459, 12183)\n"
     ]
    }
   ],
   "source": [
    "test_name = 'segerstolpe'\n",
    "if test_name == 'muraro':\n",
    "    data_train = np.hstack((baron_logcounts_share, xin_logcounts_share, segerstolpe_logcounts_share))\n",
    "if test_name == 'baron':\n",
    "    data_train = np.hstack((muraro_logcounts_share, xin_logcounts_share, segerstolpe_logcounts_share))\n",
    "if test_name == 'xin':\n",
    "    data_train = np.hstack((muraro_logcounts_share, baron_logcounts_share, segerstolpe_logcounts_share))\n",
    "if test_name == 'segerstolpe':\n",
    "    data_train = np.hstack((xin_logcounts_share, baron_logcounts_share, muraro_logcounts_share))\n",
    "muraro_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/muraro/muraro_label.csv').iloc[:,1].values.tolist()\n",
    "baron_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/baron/baron_label.csv').iloc[:,1].values.tolist()\n",
    "xin_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/xin/xin_label.csv').iloc[:,1].values.tolist()\n",
    "segerstolpe_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/segerstolpe/segerstolpe_label.csv').iloc[:,1].values.tolist()\n",
    "if test_name == 'muraro':\n",
    "    label_train = baron_label+xin_label+segerstolpe_label\n",
    "    test_logcounts_share = muraro_logcounts_share\n",
    "if test_name == 'baron':\n",
    "    label_train = muraro_label+xin_label+segerstolpe_label\n",
    "    test_logcounts_share = baron_logcounts_share\n",
    "if test_name == 'xin':\n",
    "    label_train = muraro_label+baron_label+segerstolpe_label\n",
    "    test_logcounts_share = xin_logcounts_share\n",
    "if test_name == 'segerstolpe':\n",
    "    label_train = xin_label+baron_label+muraro_label\n",
    "    test_logcounts_share = segerstolpe_logcounts_share\n",
    "hkl.dump(label_train, '/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual/data_train_label.hkl'%test_name)\n",
    "print(data_train.shape)\n",
    "filtered_data_train = np.int64(data_train>0)\n",
    "filtered_data_test = np.int64(test_logcounts_share>0)\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_SuperCT.hkl'%(test_name,100))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_SuperCT.hkl'%(test_name,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original = np.exp(baron_logcounts_share) - 1\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "data_train_var = np.var(data_original,1)\n",
    "data_train_mean_log = np.log(data_train_mean+1)\n",
    "data_train_var_log = np.log(data_train_var+1)\n",
    "ploy_reg = PolynomialFeatures(degree=2)\n",
    "x_ = ploy_reg.fit_transform(data_train_mean_log.reshape(-1,1))\n",
    "regr2 = linear_model.LinearRegression()\n",
    "regr2.fit(x_,data_train_var_log)\n",
    "y_pred = regr2.predict(x_)\n",
    "baron_scores = (data_train_var_log-y_pred).reshape(-1)\n",
    "data_original = np.exp(muraro_logcounts_share) - 1\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "data_train_var = np.var(data_original,1)\n",
    "data_train_mean_log = np.log(data_train_mean+1)\n",
    "data_train_var_log = np.log(data_train_var+1)\n",
    "ploy_reg = PolynomialFeatures(degree=2)\n",
    "x_ = ploy_reg.fit_transform(data_train_mean_log.reshape(-1,1))\n",
    "regr2 = linear_model.LinearRegression()\n",
    "regr2.fit(x_,data_train_var_log)\n",
    "y_pred = regr2.predict(x_)\n",
    "muraro_scores = (data_train_var_log-y_pred).reshape(-1)\n",
    "data_original = np.exp(xin_logcounts_share) - 1\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "data_train_var = np.var(data_original,1)\n",
    "data_train_mean_log = np.log(data_train_mean+1)\n",
    "data_train_var_log = np.log(data_train_var+1)\n",
    "ploy_reg = PolynomialFeatures(degree=2)\n",
    "x_ = ploy_reg.fit_transform(data_train_mean_log.reshape(-1,1))\n",
    "regr2 = linear_model.LinearRegression()\n",
    "regr2.fit(x_,data_train_var_log)\n",
    "y_pred = regr2.predict(x_)\n",
    "xin_scores = (data_train_var_log-y_pred).reshape(-1)\n",
    "data_original = np.exp(segerstolpe_logcounts_share) - 1\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "data_train_var = np.var(data_original,1)\n",
    "data_train_mean_log = np.log(data_train_mean+1)\n",
    "data_train_var_log = np.log(data_train_var+1)\n",
    "ploy_reg = PolynomialFeatures(degree=2)\n",
    "x_ = ploy_reg.fit_transform(data_train_mean_log.reshape(-1,1))\n",
    "regr2 = linear_model.LinearRegression()\n",
    "regr2.fit(x_,data_train_var_log)\n",
    "y_pred = regr2.predict(x_)\n",
    "segerstolpe_scores = (data_train_var_log-y_pred).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 1165\n",
    "ind_muraro = np.argpartition(muraro_scores, -K)[-K:].tolist()\n",
    "ind_baron = np.argpartition(baron_scores, -K)[-K:].tolist()\n",
    "ind_xin = np.argpartition(xin_scores, -K)[-K:].tolist()\n",
    "ind_segerstolpe = np.argpartition(segerstolpe_scores, -K)[-K:].tolist()\n",
    "\n",
    "ind_muraro_baron = intersection(ind_muraro, ind_baron)\n",
    "ind_muraro_baron_xin = intersection(ind_muraro_baron, ind_xin)\n",
    "ind_muraro_baron_xin_segerstolpe = intersection(ind_muraro_baron_xin, ind_segerstolpe)\n",
    "len(ind_muraro_baron_xin_segerstolpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = 'segerstolpe'\n",
    "if test_name == 'muraro':\n",
    "    data_train = np.hstack((baron_logcounts_share, xin_logcounts_share, segerstolpe_logcounts_share))\n",
    "if test_name == 'baron':\n",
    "    data_train = np.hstack((muraro_logcounts_share, xin_logcounts_share, segerstolpe_logcounts_share))\n",
    "if test_name == 'xin':\n",
    "    data_train = np.hstack((muraro_logcounts_share, baron_logcounts_share, segerstolpe_logcounts_share))\n",
    "if test_name == 'segerstolpe':\n",
    "    data_train = np.hstack((xin_logcounts_share, baron_logcounts_share, muraro_logcounts_share))\n",
    "muraro_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/muraro/muraro_label.csv').iloc[:,1].values.tolist()\n",
    "baron_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/baron/baron_label.csv').iloc[:,1].values.tolist()\n",
    "xin_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/xin/xin_label.csv').iloc[:,1].values.tolist()\n",
    "segerstolpe_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/segerstolpe/segerstolpe_label.csv').iloc[:,1].values.tolist()\n",
    "if test_name == 'muraro':\n",
    "    label_train = baron_label+xin_label+segerstolpe_label\n",
    "    test_logcounts_share = muraro_logcounts_share\n",
    "if test_name == 'baron':\n",
    "    label_train = muraro_label+xin_label+segerstolpe_label\n",
    "    test_logcounts_share = baron_logcounts_share\n",
    "if test_name == 'xin':\n",
    "    label_train = muraro_label+baron_label+segerstolpe_label\n",
    "    test_logcounts_share = xin_logcounts_share\n",
    "if test_name == 'segerstolpe':\n",
    "    label_train = xin_label+baron_label+muraro_label\n",
    "    test_logcounts_share = segerstolpe_logcounts_share\n",
    "hkl.dump(label_train, '/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual/data_train_label.hkl'%test_name)\n",
    "filtered_data_train = data_train[ind_muraro_baron_xin_segerstolpe, :]\n",
    "filtered_data_test = test_logcounts_share[ind_muraro_baron_xin_segerstolpe, :]\n",
    "K = 100\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_460147.hkl'%(test_name,K))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_460147.hkl'%(test_name,K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = 'muraro'\n",
    "if test_name == 'muraro':\n",
    "    data_train = np.hstack((baron_logcounts_share, xin_logcounts_share, segerstolpe_logcounts_share))\n",
    "if test_name == 'baron':\n",
    "    data_train = np.hstack((muraro_logcounts_share, xin_logcounts_share, segerstolpe_logcounts_share))\n",
    "if test_name == 'xin':\n",
    "    data_train = np.hstack((muraro_logcounts_share, baron_logcounts_share, segerstolpe_logcounts_share))\n",
    "if test_name == 'segerstolpe':\n",
    "    data_train = np.hstack((xin_logcounts_share, baron_logcounts_share, muraro_logcounts_share))\n",
    "if test_name == 'muraro':\n",
    "    label_train = baron_label+xin_label+segerstolpe_label\n",
    "    test_logcounts_share = muraro_logcounts_share\n",
    "if test_name == 'baron':\n",
    "    label_train = muraro_label+xin_label+segerstolpe_label\n",
    "    test_logcounts_share = baron_logcounts_share\n",
    "if test_name == 'xin':\n",
    "    label_train = muraro_label+baron_label+segerstolpe_label\n",
    "    test_logcounts_share = xin_logcounts_share\n",
    "if test_name == 'segerstolpe':\n",
    "    label_train = xin_label+baron_label+muraro_label\n",
    "    test_logcounts_share = segerstolpe_logcounts_share\n",
    "hkl.dump(label_train, '/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual/data_train_label.hkl'%test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train = min_max_scaler.fit_transform(data_train.T)\n",
    "X_test = min_max_scaler.transform(test_logcounts_share.T)\n",
    "data = np.hstack((muraro_logcounts_share, baron_logcounts_share, xin_logcounts_share, segerstolpe_logcounts_share))\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_ = min_max_scaler.fit_transform(data.T)\n",
    "X_train = min_max_scaler.transform(data_train.T)\n",
    "X_test = min_max_scaler.transform(test_logcounts_share.T)\n",
    "pca=PCA(n_components=100)\n",
    "pca.fit(X_)\n",
    "PCA_X_train = pca.transform(X_train)\n",
    "PCA_X_test = pca.transform(X_test)\n",
    "hkl.dump(PCA_X_train, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_PCA.hkl'%(test_name,100))\n",
    "hkl.dump(PCA_X_test, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_PCA.hkl'%(test_name,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myself\n",
    "muraro_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(muraro_logcounts_share) - 1\n",
    "drop_feature = []\n",
    "X1 = []\n",
    "Y = []\n",
    "X2 = []\n",
    "feature_index_dict = []\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        Y.append(np.log(data_train_mean[i]+1))\n",
    "        X1.append(np.mean(muraro_logcounts_share[i,:]))\n",
    "        X2.append(np.log(drop_feature[i]))\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "X1 = np.array(X1).reshape(-1,1)\n",
    "X2 = np.array(X2).reshape(-1,1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X2,Y)\n",
    "predictions = lr.predict(X2)\n",
    "residuals = (Y-predictions).reshape(-1) + (Y - X1).reshape(-1)\n",
    "muraro_scores[feature_index_dict] = residuals\n",
    "baron_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(baron_logcounts_share) - 1\n",
    "drop_feature = []\n",
    "X1 = []\n",
    "Y = []\n",
    "X2 = []\n",
    "feature_index_dict = []\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        Y.append(np.log(data_train_mean[i]+1))\n",
    "        X1.append(np.mean(baron_logcounts_share[i,:]))\n",
    "        X2.append(np.log(drop_feature[i]))\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "X1 = np.array(X1).reshape(-1,1)\n",
    "X2 = np.array(X2).reshape(-1,1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X2,Y)\n",
    "predictions = lr.predict(X2)\n",
    "residuals = (Y-predictions).reshape(-1) + (Y - X1).reshape(-1)\n",
    "baron_scores[feature_index_dict] = residuals\n",
    "xin_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(xin_logcounts_share) - 1\n",
    "drop_feature = []\n",
    "X1 = []\n",
    "Y = []\n",
    "X2 = []\n",
    "feature_index_dict = []\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        Y.append(np.log(data_train_mean[i]+1))\n",
    "        X1.append(np.mean(xin_logcounts_share[i,:]))\n",
    "        X2.append(np.log(drop_feature[i]))\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "X1 = np.array(X1).reshape(-1,1)\n",
    "X2 = np.array(X2).reshape(-1,1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X2,Y)\n",
    "predictions = lr.predict(X2)\n",
    "residuals = (Y-predictions).reshape(-1) + (Y - X1).reshape(-1)\n",
    "xin_scores[feature_index_dict] = residuals\n",
    "segerstolpe_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(segerstolpe_logcounts_share) - 1\n",
    "drop_feature = []\n",
    "X1 = []\n",
    "Y = []\n",
    "X2 = []\n",
    "feature_index_dict = []\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        Y.append(np.log(data_train_mean[i]+1))\n",
    "        X1.append(np.mean(segerstolpe_logcounts_share[i,:]))\n",
    "        X2.append(np.log(drop_feature[i]))\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "X1 = np.array(X1).reshape(-1,1)\n",
    "X2 = np.array(X2).reshape(-1,1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X2,Y)\n",
    "predictions = lr.predict(X2)\n",
    "residuals = (Y-predictions).reshape(-1) + (Y - X1).reshape(-1)\n",
    "segerstolpe_scores[feature_index_dict] = residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 3260\n",
    "ind_muraro = np.argpartition(muraro_scores, -K)[-K:].tolist()\n",
    "ind_baron = np.argpartition(baron_scores, -K)[-K:].tolist()\n",
    "ind_xin = np.argpartition(xin_scores, -K)[-K:].tolist()\n",
    "ind_segerstolpe = np.argpartition(segerstolpe_scores, -K)[-K:].tolist()\n",
    "\n",
    "ind_muraro_baron = intersection(ind_muraro, ind_baron)\n",
    "ind_muraro_baron_xin = intersection(ind_muraro_baron, ind_xin)\n",
    "ind_muraro_baron_xin_segerstolpe = intersection(ind_muraro_baron_xin, ind_segerstolpe)\n",
    "len(ind_muraro_baron_xin_segerstolpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "muraro_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/muraro/muraro_label.csv').iloc[:,1].values.tolist()\n",
    "baron_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/baron/baron_label.csv').iloc[:,1].values.tolist()\n",
    "xin_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/xin/xin_label.csv').iloc[:,1].values.tolist()\n",
    "segerstolpe_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/segerstolpe/segerstolpe_label.csv').iloc[:,1].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = 'segerstolpe'\n",
    "if test_name == 'muraro':\n",
    "    data_train = np.hstack((baron_logcounts_share, xin_logcounts_share, segerstolpe_logcounts_share))\n",
    "if test_name == 'baron':\n",
    "    data_train = np.hstack((muraro_logcounts_share, xin_logcounts_share, segerstolpe_logcounts_share))\n",
    "if test_name == 'xin':\n",
    "    data_train = np.hstack((muraro_logcounts_share, baron_logcounts_share, segerstolpe_logcounts_share))\n",
    "if test_name == 'segerstolpe':\n",
    "    data_train = np.hstack((xin_logcounts_share, baron_logcounts_share, muraro_logcounts_share))\n",
    "if test_name == 'muraro':\n",
    "    label_train = baron_label+xin_label+segerstolpe_label\n",
    "    test_logcounts_share = muraro_logcounts_share\n",
    "if test_name == 'baron':\n",
    "    label_train = muraro_label+xin_label+segerstolpe_label\n",
    "    test_logcounts_share = baron_logcounts_share\n",
    "if test_name == 'xin':\n",
    "    label_train = muraro_label+baron_label+segerstolpe_label\n",
    "    test_logcounts_share = xin_logcounts_share\n",
    "if test_name == 'segerstolpe':\n",
    "    label_train = xin_label+baron_label+muraro_label\n",
    "    test_logcounts_share = segerstolpe_logcounts_share\n",
    "hkl.dump(label_train, '/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual/data_train_label.hkl'%test_name)\n",
    "filtered_data_train = data_train[ind_muraro_baron_xin_segerstolpe, :]\n",
    "filtered_data_test = test_logcounts_share[ind_muraro_baron_xin_segerstolpe, :]\n",
    "K = len(ind_muraro_baron_xin_segerstolpe)\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_myself.hkl'%(test_name,K))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_myself.hkl'%(test_name,K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#同类异源\n",
    "#小鼠视网膜=>小鼠脑细胞\n",
    "#小鼠脑细胞=>小鼠视网膜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "shekhar_feature = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/shekhar/shekhar_feature.csv').iloc[:,1].values.tolist()\n",
    "macosko_feature = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/macosko/macosko_feature.csv').iloc[:,1].values.tolist()\n",
    "romanov_feature = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/romanov/romanov_feature.csv').iloc[:,1].values.tolist()\n",
    "zeisel_feature = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/zeisel/zeisel_feature.csv').iloc[:,1].values.tolist()\n",
    "\n",
    "shekhar_macosko = intersection(shekhar_feature, macosko_feature)\n",
    "shekhar_macosko_romanov = intersection(shekhar_macosko, romanov_feature)\n",
    "shekhar_macosko_romanov_zeisel = intersection(shekhar_macosko_romanov, zeisel_feature)\n",
    "shekhar_feature_share = [shekhar_feature.index(i) for i in shekhar_macosko_romanov_zeisel]\n",
    "macosko_feature_share = [macosko_feature.index(i) for i in shekhar_macosko_romanov_zeisel]\n",
    "romanov_feature_share = [romanov_feature.index(i) for i in shekhar_macosko_romanov_zeisel]\n",
    "zeisel_feature_share = [zeisel_feature.index(i) for i in shekhar_macosko_romanov_zeisel]\n",
    "\n",
    "shekhar_logcounts = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/shekhar/shekhar_logcounts.csv').iloc[:,1:].values\n",
    "macosko_logcounts = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/macosko/macosko_logcounts.csv').iloc[:,1:].values\n",
    "romanov_logcounts = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/romanov/romanov_logcounts.csv').iloc[:,1:].values\n",
    "zeisel_logcounts = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/zeisel/zeisel_logcounts.csv').iloc[:,1:].values\n",
    "\n",
    "shekhar_logcounts_share = shekhar_logcounts[shekhar_feature_share, :]\n",
    "macosko_logcounts_share = macosko_logcounts[macosko_feature_share, :]\n",
    "romanov_logcounts_share = romanov_logcounts[romanov_feature_share, :]\n",
    "zeisel_logcounts_share = zeisel_logcounts[zeisel_feature_share, :]\n",
    "\n",
    "feature_number = len(shekhar_macosko_romanov_zeisel)\n",
    "shekhar_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/shekhar/shekhar_label.csv').iloc[:,1].values.tolist()\n",
    "macosko_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/macosko/macosko_label.csv').iloc[:,1].values.tolist()\n",
    "romanov_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/romanov/romanov_label.csv').iloc[:,1].values.tolist()\n",
    "zeisel_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/zeisel/zeisel_label.csv').iloc[:,1].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(romanov_label)):\n",
    "    if romanov_label[i] == 'oligos':\n",
    "        romanov_label[i] = 'oligodendrocytes'\n",
    "for i in range(len(zeisel_label)):\n",
    "    if zeisel_label[i] == 'ca1pyramidal':\n",
    "        zeisel_label[i] = 'neurons'\n",
    "    if zeisel_label[i] == 's1pyramidal':\n",
    "        zeisel_label[i] = 'neurons'\n",
    "    if zeisel_label[i] == 'interneurons':\n",
    "        zeisel_label[i] = 'neurons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.hstack((romanov_logcounts_share,zeisel_logcounts_share))\n",
    "label_train = romanov_label+zeisel_label\n",
    "test_logcounts_share = np.hstack((shekhar_logcounts_share,macosko_logcounts_share))\n",
    "label_test = shekhar_label+macosko_label\n",
    "test_name = \"retina\"\n",
    "if not os.path.exists('/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual'%test_name):\n",
    "    os.makedirs('/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual'%test_name)\n",
    "hkl.dump(label_train, '/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual/data_train_label.hkl'%test_name)\n",
    "hkl.dump(label_test, '/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual/data_test_label.hkl'%test_name)\n",
    "test_name = \"brain\"\n",
    "if not os.path.exists('/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual'%test_name):\n",
    "    os.makedirs('/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual'%test_name)\n",
    "hkl.dump(label_test, '/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual/data_train_label.hkl'%test_name)\n",
    "hkl.dump(label_train, '/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual/data_test_label.hkl'%test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = \"retina\"\n",
    "filtered_data_train = np.int64(data_train>0)\n",
    "filtered_data_test = np.int64(test_logcounts_share>0)\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_SuperCT.hkl'%(test_name,100))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_SuperCT.hkl'%(test_name,100))\n",
    "test_name = \"brain\"\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_SuperCT.hkl'%(test_name,100))\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_SuperCT.hkl'%(test_name,100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11726,)\n",
      "(11732,)\n",
      "(11732,)\n",
      "(11732,)\n"
     ]
    }
   ],
   "source": [
    "#scmap\n",
    "romanov_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(romanov_logcounts_share) - 1\n",
    "data_original_mean = np.mean(data_original,1)\n",
    "drop_feature = []\n",
    "\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "#print(drop_feature)\n",
    "feature_index_dict = []\n",
    "train_E = []\n",
    "train_D = []\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        train_E.append(data_original_mean[i])\n",
    "        train_D.append(drop_feature[i])\n",
    "train_E = np.array(train_E)\n",
    "train_D = np.array(train_D)\n",
    "train_E = np.log(train_E+1).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "train_D = np.log(100*train_D).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_E,train_D)\n",
    "predictions = lr.predict(train_E)\n",
    "residuals = (train_D-predictions).reshape(-1)\n",
    "#residuals = np.abs(residuals)\n",
    "romanov_scores[feature_index_dict] = residuals\n",
    "print(residuals.shape)\n",
    "\n",
    "zeisel_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(zeisel_logcounts_share) - 1\n",
    "data_original_mean = np.mean(data_original,1)\n",
    "drop_feature = []\n",
    "\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "#print(drop_feature)\n",
    "feature_index_dict = []\n",
    "train_E = []\n",
    "train_D = []\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        train_E.append(data_original_mean[i])\n",
    "        train_D.append(drop_feature[i])\n",
    "train_E = np.array(train_E)\n",
    "train_D = np.array(train_D)\n",
    "train_E = np.log(train_E+1).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "train_D = np.log(100*train_D).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_E,train_D)\n",
    "predictions = lr.predict(train_E)\n",
    "residuals = (train_D-predictions).reshape(-1)\n",
    "#residuals = np.abs(residuals)\n",
    "zeisel_scores[feature_index_dict] = residuals\n",
    "print(residuals.shape)\n",
    "shekhar_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(shekhar_logcounts_share) - 1\n",
    "data_original_mean = np.mean(data_original,1)\n",
    "drop_feature = []\n",
    "\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "#print(drop_feature)\n",
    "feature_index_dict = []\n",
    "train_E = []\n",
    "train_D = []\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        train_E.append(data_original_mean[i])\n",
    "        train_D.append(drop_feature[i])\n",
    "train_E = np.array(train_E)\n",
    "train_D = np.array(train_D)\n",
    "train_E = np.log(train_E+1).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "train_D = np.log(100*train_D).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_E,train_D)\n",
    "predictions = lr.predict(train_E)\n",
    "residuals = (train_D-predictions).reshape(-1)\n",
    "#residuals = np.abs(residuals)\n",
    "shekhar_scores[feature_index_dict] = residuals\n",
    "print(residuals.shape)\n",
    "\n",
    "macosko_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(macosko_logcounts_share) - 1\n",
    "data_original_mean = np.mean(data_original,1)\n",
    "drop_feature = []\n",
    "\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "#print(drop_feature)\n",
    "feature_index_dict = []\n",
    "train_E = []\n",
    "train_D = []\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        train_E.append(data_original_mean[i])\n",
    "        train_D.append(drop_feature[i])\n",
    "train_E = np.array(train_E)\n",
    "train_D = np.array(train_D)\n",
    "train_E = np.log(train_E+1).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "train_D = np.log(100*train_D).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_E,train_D)\n",
    "predictions = lr.predict(train_E)\n",
    "residuals = (train_D-predictions).reshape(-1)\n",
    "#residuals = np.abs(residuals)\n",
    "macosko_scores[feature_index_dict] = residuals\n",
    "print(residuals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "K = 1140\n",
    "ind_romanov = np.argpartition(romanov_scores, -K)[-K:].tolist()\n",
    "ind_zeisel = np.argpartition(zeisel_scores, -K)[-K:].tolist()\n",
    "ind_shekhar = np.argpartition(shekhar_scores, -K)[-K:].tolist()\n",
    "ind_macosko = np.argpartition(macosko_scores, -K)[-K:].tolist()\n",
    "ind_romanov_zeisel = intersection(ind_romanov, ind_zeisel)\n",
    "ind_romanov_zeisel_shekhar = intersection(ind_romanov_zeisel, ind_shekhar)\n",
    "ind_romanov_zeisel_shekhar_macosko = intersection(ind_romanov_zeisel_shekhar, ind_macosko)\n",
    "print(len(ind_romanov_zeisel_shekhar_macosko))\n",
    "filtered_data_train = data_train[ind_romanov_zeisel_shekhar_macosko, :]\n",
    "filtered_data_test = test_logcounts_share[ind_romanov_zeisel_shekhar_macosko, :]\n",
    "K = 100\n",
    "test_name = \"retina\"\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_scmap.hkl'%(test_name,K))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_scmap.hkl'%(test_name,K))\n",
    "test_name = \"brain\"\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_scmap.hkl'%(test_name,K))\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_scmap.hkl'%(test_name,K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.hstack((romanov_logcounts_share,zeisel_logcounts_share))\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_ = min_max_scaler.fit_transform(data.T)\n",
    "X_train = min_max_scaler.transform(data_train.T)\n",
    "X_test = min_max_scaler.transform(test_logcounts_share.T)\n",
    "pca=PCA(n_components=100)\n",
    "pca.fit(X_)\n",
    "PCA_X_train = pca.transform(X_train)\n",
    "PCA_X_test = pca.transform(X_test)\n",
    "test_name = \"retina\"\n",
    "hkl.dump(PCA_X_train, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_PCA.hkl'%(test_name,100))\n",
    "hkl.dump(PCA_X_test, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_PCA.hkl'%(test_name,100))\n",
    "test_name = \"brain\"\n",
    "hkl.dump(PCA_X_test, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_PCA.hkl'%(test_name,100))\n",
    "hkl.dump(PCA_X_train, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_PCA.hkl'%(test_name,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myself 自己方法\n",
    "zeisel_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(zeisel_logcounts_share) - 1\n",
    "drop_feature = []\n",
    "X1 = []\n",
    "Y = []\n",
    "X2 = []\n",
    "feature_index_dict = []\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        Y.append(np.log(data_train_mean[i]+1))\n",
    "        X1.append(np.mean(zeisel_logcounts_share[i,:]))\n",
    "        X2.append(np.log(drop_feature[i]))\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "X1 = np.array(X1).reshape(-1,1)\n",
    "X2 = np.array(X2).reshape(-1,1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X2,Y)\n",
    "predictions = lr.predict(X2)\n",
    "residuals = (Y-predictions).reshape(-1) + (Y - X1).reshape(-1)\n",
    "zeisel_scores[feature_index_dict] = residuals\n",
    "romanov_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(romanov_logcounts_share) - 1\n",
    "drop_feature = []\n",
    "X1 = []\n",
    "Y = []\n",
    "X2 = []\n",
    "feature_index_dict = []\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        Y.append(np.log(data_train_mean[i]+1))\n",
    "        X1.append(np.mean(romanov_logcounts_share[i,:]))\n",
    "        X2.append(np.log(drop_feature[i]))\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "X1 = np.array(X1).reshape(-1,1)\n",
    "X2 = np.array(X2).reshape(-1,1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X2,Y)\n",
    "predictions = lr.predict(X2)\n",
    "residuals = (Y-predictions).reshape(-1) + (Y - X1).reshape(-1)\n",
    "romanov_scores[feature_index_dict] = residuals\n",
    "macosko_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(macosko_logcounts_share) - 1\n",
    "drop_feature = []\n",
    "X1 = []\n",
    "Y = []\n",
    "X2 = []\n",
    "feature_index_dict = []\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        Y.append(np.log(data_train_mean[i]+1))\n",
    "        X1.append(np.mean(macosko_logcounts_share[i,:]))\n",
    "        X2.append(np.log(drop_feature[i]))\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "X1 = np.array(X1).reshape(-1,1)\n",
    "X2 = np.array(X2).reshape(-1,1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X2,Y)\n",
    "predictions = lr.predict(X2)\n",
    "residuals = (Y-predictions).reshape(-1) + (Y - X1).reshape(-1)\n",
    "macosko_scores[feature_index_dict] = residuals\n",
    "shekhar_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(shekhar_logcounts_share) - 1\n",
    "drop_feature = []\n",
    "X1 = []\n",
    "Y = []\n",
    "X2 = []\n",
    "feature_index_dict = []\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        Y.append(np.log(data_train_mean[i]+1))\n",
    "        X1.append(np.mean(shekhar_logcounts_share[i,:]))\n",
    "        X2.append(np.log(drop_feature[i]))\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "X1 = np.array(X1).reshape(-1,1)\n",
    "X2 = np.array(X2).reshape(-1,1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X2,Y)\n",
    "predictions = lr.predict(X2)\n",
    "residuals = (Y-predictions).reshape(-1) + (Y - X1).reshape(-1)\n",
    "shekhar_scores[feature_index_dict] = residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "K = 990\n",
    "ind_romanov = np.argpartition(romanov_scores, -K)[-K:].tolist()\n",
    "ind_zeisel = np.argpartition(zeisel_scores, -K)[-K:].tolist()\n",
    "ind_shekhar = np.argpartition(shekhar_scores, -K)[-K:].tolist()\n",
    "ind_macosko = np.argpartition(macosko_scores, -K)[-K:].tolist()\n",
    "ind_romanov_zeisel = intersection(ind_romanov, ind_zeisel)\n",
    "ind_romanov_zeisel_shekhar = intersection(ind_romanov_zeisel, ind_shekhar)\n",
    "ind_romanov_zeisel_shekhar_macosko = intersection(ind_romanov_zeisel_shekhar, ind_macosko)\n",
    "print(len(ind_romanov_zeisel_shekhar_macosko))\n",
    "filtered_data_train = data_train[ind_romanov_zeisel_shekhar_macosko, :]\n",
    "filtered_data_test = test_logcounts_share[ind_romanov_zeisel_shekhar_macosko, :]\n",
    "K = 100\n",
    "test_name = \"retina\"\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_myself.hkl'%(test_name,K))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_myself.hkl'%(test_name,K))\n",
    "test_name = \"brain\"\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_myself.hkl'%(test_name,K))\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_myself.hkl'%(test_name,K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original = np.exp(romanov_logcounts_share) - 1\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "data_train_var = np.var(data_original,1)\n",
    "data_train_mean_log = np.log(data_train_mean+1)\n",
    "data_train_var_log = np.log(data_train_var+1)\n",
    "ploy_reg = PolynomialFeatures(degree=2)\n",
    "x_ = ploy_reg.fit_transform(data_train_mean_log.reshape(-1,1))\n",
    "regr2 = linear_model.LinearRegression()\n",
    "regr2.fit(x_,data_train_var_log)\n",
    "y_pred = regr2.predict(x_)\n",
    "romanov_scores = (data_train_var_log-y_pred).reshape(-1)\n",
    "data_original = np.exp(zeisel_logcounts_share) - 1\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "data_train_var = np.var(data_original,1)\n",
    "data_train_mean_log = np.log(data_train_mean+1)\n",
    "data_train_var_log = np.log(data_train_var+1)\n",
    "ploy_reg = PolynomialFeatures(degree=2)\n",
    "x_ = ploy_reg.fit_transform(data_train_mean_log.reshape(-1,1))\n",
    "regr2 = linear_model.LinearRegression()\n",
    "regr2.fit(x_,data_train_var_log)\n",
    "y_pred = regr2.predict(x_)\n",
    "zeisel_scores = (data_train_var_log-y_pred).reshape(-1)\n",
    "data_original = np.exp(shekhar_logcounts_share) - 1\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "data_train_var = np.var(data_original,1)\n",
    "data_train_mean_log = np.log(data_train_mean+1)\n",
    "data_train_var_log = np.log(data_train_var+1)\n",
    "ploy_reg = PolynomialFeatures(degree=2)\n",
    "x_ = ploy_reg.fit_transform(data_train_mean_log.reshape(-1,1))\n",
    "regr2 = linear_model.LinearRegression()\n",
    "regr2.fit(x_,data_train_var_log)\n",
    "y_pred = regr2.predict(x_)\n",
    "shekhar_scores = (data_train_var_log-y_pred).reshape(-1)\n",
    "data_original = np.exp(macosko_logcounts_share) - 1\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "data_train_var = np.var(data_original,1)\n",
    "data_train_mean_log = np.log(data_train_mean+1)\n",
    "data_train_var_log = np.log(data_train_var+1)\n",
    "ploy_reg = PolynomialFeatures(degree=2)\n",
    "x_ = ploy_reg.fit_transform(data_train_mean_log.reshape(-1,1))\n",
    "regr2 = linear_model.LinearRegression()\n",
    "regr2.fit(x_,data_train_var_log)\n",
    "y_pred = regr2.predict(x_)\n",
    "macosko_scores = (data_train_var_log-y_pred).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "K = 1450\n",
    "ind_romanov = np.argpartition(romanov_scores, -K)[-K:].tolist()\n",
    "ind_zeisel = np.argpartition(zeisel_scores, -K)[-K:].tolist()\n",
    "ind_shekhar = np.argpartition(shekhar_scores, -K)[-K:].tolist()\n",
    "ind_macosko = np.argpartition(macosko_scores, -K)[-K:].tolist()\n",
    "ind_romanov_zeisel = intersection(ind_romanov, ind_zeisel)\n",
    "ind_romanov_zeisel_shekhar = intersection(ind_romanov_zeisel, ind_shekhar)\n",
    "ind_romanov_zeisel_shekhar_macosko = intersection(ind_romanov_zeisel_shekhar, ind_macosko)\n",
    "print(len(ind_romanov_zeisel_shekhar_macosko))\n",
    "filtered_data_train = data_train[ind_romanov_zeisel_shekhar_macosko, :]\n",
    "filtered_data_test = test_logcounts_share[ind_romanov_zeisel_shekhar_macosko, :]\n",
    "K = 100\n",
    "test_name = \"retina\"\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_myself.hkl'%(test_name,K))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_myself.hkl'%(test_name,K))\n",
    "test_name = \"brain\"\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_myself.hkl'%(test_name,K))\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_myself.hkl'%(test_name,K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baron2\n",
    "muraro_feature = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/muraro/muraro_feature.csv').iloc[:,1].values.tolist()\n",
    "baron_feature = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/baron/baron_feature.csv').iloc[:,1].values.tolist()\n",
    "xin_feature = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/xin/xin_feature.csv').iloc[:,1].values.tolist()\n",
    "segerstolpe_feature = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/segerstolpe/segerstolpe_feature.csv').iloc[:,1].values.tolist()\n",
    "baron2_feature = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/baron2/baron2_feature.csv').iloc[:,1].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baron2_feature = [i.upper() for  i in baron2_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baron2专用\n",
    "test_name = 'baron2'\n",
    "#找到共同基因集合\n",
    "muraro_baron = intersection(muraro_feature, baron_feature)\n",
    "muraro_baron_xin = intersection(muraro_baron, xin_feature)\n",
    "muraro_baron_xin_segerstolpe = intersection(muraro_baron_xin, segerstolpe_feature)\n",
    "muraro_baron_xin_segerstolpe_baron2 = intersection(muraro_baron_xin_segerstolpe, baron2_feature)\n",
    "#找到这些相同基因位置\n",
    "muraro_feature_share = [muraro_feature.index(i) for i in muraro_baron_xin_segerstolpe_baron2]\n",
    "baron_feature_share = [baron_feature.index(i) for i in muraro_baron_xin_segerstolpe_baron2]\n",
    "xin_feature_share = [xin_feature.index(i) for i in muraro_baron_xin_segerstolpe_baron2]\n",
    "segerstolpe_feature_share = [segerstolpe_feature.index(i) for i in muraro_baron_xin_segerstolpe_baron2]\n",
    "baron2_feature_share = [baron2_feature.index(i) for i in muraro_baron_xin_segerstolpe_baron2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据集\n",
    "muraro_logcounts = pd.read_csv('/home/chenshengquan/data/scmap/muraro_logcounts.csv').iloc[:,1:].values\n",
    "baron_logcounts = pd.read_csv('/home/chenshengquan/data/scmap/baron_logcounts.csv').iloc[:,1:].values\n",
    "xin_logcounts = pd.read_csv('/home/chenshengquan/data/scmap/xin_logcounts.csv').iloc[:,1:].values\n",
    "segerstolpe_logcounts = pd.read_csv('/home/chenshengquan/data/scmap/segerstolpe_logcounts.csv').iloc[:,1:].values\n",
    "baron2_logcounts = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/baron2/baron2_logcounts.csv').iloc[:,1:].values\n",
    "#所用数据集（未进行降维）\n",
    "muraro_logcounts_share = muraro_logcounts[muraro_feature_share, :]\n",
    "baron_logcounts_share = baron_logcounts[baron_feature_share, :]\n",
    "xin_logcounts_share = xin_logcounts[xin_feature_share, :]\n",
    "segerstolpe_logcounts_share = segerstolpe_logcounts[segerstolpe_feature_share, :]\n",
    "baron2_logcounts_share = baron2_logcounts[baron2_feature_share,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "muraro_label = pd.read_csv('/home/chenshengquan/data/scmap/muraro_label.csv').iloc[:,1].values.tolist()\n",
    "baron_label = pd.read_csv('/home/chenshengquan/data/scmap/baron_label.csv').iloc[:,1].values.tolist()\n",
    "xin_label = pd.read_csv('/home/chenshengquan/data/scmap/xin_label.csv').iloc[:,1].values.tolist()\n",
    "segerstolpe_label = pd.read_csv('/home/chenshengquan/data/scmap/segerstolpe_label.csv').iloc[:,1].values.tolist()\n",
    "baron2_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/baron2/baron2_label.csv').iloc[:,1].values.tolist()\n",
    "label_train = muraro_label+baron_label+xin_label+segerstolpe_label\n",
    "label_test = baron2_label\n",
    "hkl.dump(label_train, '/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual/data_train_label.hkl'%test_name)\n",
    "hkl.dump(label_test, '/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual/data_test_label.hkl'%test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.hstack((muraro_logcounts_share,baron_logcounts_share, xin_logcounts_share, segerstolpe_logcounts_share))\n",
    "test_logcounts_share = baron2_logcounts_share\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "beta                  894\n",
       "ductal                275\n",
       "delta                 218\n",
       "alpha                 191\n",
       "endothelial           139\n",
       "quiescent_stellate     47\n",
       "gamma                  41\n",
       "macrophage             36\n",
       "activated_stellate     14\n",
       "B_cell                 10\n",
       "immune_other            8\n",
       "T_cell                  7\n",
       "schwann                 6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(baron2_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = []\n",
    "k = 0\n",
    "for i in range(len(baron2_label)):\n",
    "    if baron2_label[i] in label_train:\n",
    "        k = k+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.986744432661718"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k/len(baron2_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11948"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(muraro_baron_xin_segerstolpe_baron2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "muraro_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(muraro_logcounts_share) - 1\n",
    "drop_feature = []\n",
    "X1 = []\n",
    "Y = []\n",
    "X2 = []\n",
    "feature_index_dict = []\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        Y.append(np.log(data_train_mean[i]+1))\n",
    "        X1.append(np.mean(muraro_logcounts_share[i,:]))\n",
    "        X2.append(np.log(drop_feature[i]))\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "X1 = np.array(X1).reshape(-1,1)\n",
    "X2 = np.array(X2).reshape(-1,1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X2,Y)\n",
    "predictions = lr.predict(X2)\n",
    "residuals = (Y-predictions).reshape(-1) + (Y - X1).reshape(-1)\n",
    "muraro_scores[feature_index_dict] = residuals\n",
    "baron_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(baron_logcounts_share) - 1\n",
    "drop_feature = []\n",
    "X1 = []\n",
    "Y = []\n",
    "X2 = []\n",
    "feature_index_dict = []\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        Y.append(np.log(data_train_mean[i]+1))\n",
    "        X1.append(np.mean(baron_logcounts_share[i,:]))\n",
    "        X2.append(np.log(drop_feature[i]))\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "X1 = np.array(X1).reshape(-1,1)\n",
    "X2 = np.array(X2).reshape(-1,1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X2,Y)\n",
    "predictions = lr.predict(X2)\n",
    "residuals = (Y-predictions).reshape(-1) + (Y - X1).reshape(-1)\n",
    "baron_scores[feature_index_dict] = residuals\n",
    "xin_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(xin_logcounts_share) - 1\n",
    "drop_feature = []\n",
    "X1 = []\n",
    "Y = []\n",
    "X2 = []\n",
    "feature_index_dict = []\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        Y.append(np.log(data_train_mean[i]+1))\n",
    "        X1.append(np.mean(xin_logcounts_share[i,:]))\n",
    "        X2.append(np.log(drop_feature[i]))\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "X1 = np.array(X1).reshape(-1,1)\n",
    "X2 = np.array(X2).reshape(-1,1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X2,Y)\n",
    "predictions = lr.predict(X2)\n",
    "residuals = (Y-predictions).reshape(-1) + (Y - X1).reshape(-1)\n",
    "xin_scores[feature_index_dict] = residuals\n",
    "segerstolpe_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(segerstolpe_logcounts_share) - 1\n",
    "drop_feature = []\n",
    "X1 = []\n",
    "Y = []\n",
    "X2 = []\n",
    "feature_index_dict = []\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        Y.append(np.log(data_train_mean[i]+1))\n",
    "        X1.append(np.mean(segerstolpe_logcounts_share[i,:]))\n",
    "        X2.append(np.log(drop_feature[i]))\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "X1 = np.array(X1).reshape(-1,1)\n",
    "X2 = np.array(X2).reshape(-1,1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X2,Y)\n",
    "predictions = lr.predict(X2)\n",
    "residuals = (Y-predictions).reshape(-1) + (Y - X1).reshape(-1)\n",
    "segerstolpe_scores[feature_index_dict] = residuals\n",
    "baron2_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(baron2_logcounts_share) - 1\n",
    "drop_feature = []\n",
    "X1 = []\n",
    "Y = []\n",
    "X2 = []\n",
    "feature_index_dict = []\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        Y.append(np.log(data_train_mean[i]+1))\n",
    "        X1.append(np.mean(baron2_logcounts_share[i,:]))\n",
    "        X2.append(np.log(drop_feature[i]))\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "X1 = np.array(X1).reshape(-1,1)\n",
    "X2 = np.array(X2).reshape(-1,1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X2,Y)\n",
    "predictions = lr.predict(X2)\n",
    "residuals = (Y-predictions).reshape(-1) + (Y - X1).reshape(-1)\n",
    "baron2_scores[feature_index_dict] = residuals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "K = 1945\n",
    "ind_muraro = np.argpartition(muraro_scores, -K)[-K:].tolist()\n",
    "ind_baron = np.argpartition(baron_scores, -K)[-K:].tolist()\n",
    "ind_xin = np.argpartition(xin_scores, -K)[-K:].tolist()\n",
    "ind_segerstolpe = np.argpartition(segerstolpe_scores, -K)[-K:].tolist()\n",
    "ind_baron2 = np.argpartition(baron2_scores, -K)[-K:].tolist()\n",
    "\n",
    "ind_muraro_baron = intersection(ind_muraro, ind_baron)\n",
    "ind_muraro_baron_xin = intersection(ind_muraro_baron, ind_xin)\n",
    "ind_muraro_baron_xin_segerstolpe = intersection(ind_muraro_baron_xin, ind_segerstolpe)\n",
    "ind_muraro_baron_xin_segerstolpe_baron2 =  intersection(ind_muraro_baron_xin_segerstolpe,ind_baron2)\n",
    "print(len(ind_muraro_baron_xin_segerstolpe_baron2))\n",
    "filtered_data_train = data_train[ind_muraro_baron_xin_segerstolpe_baron2, :]\n",
    "filtered_data_test = test_logcounts_share[ind_muraro_baron_xin_segerstolpe_baron2, :]\n",
    "K = 100\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_myself.hkl'%(test_name,K))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_myself.hkl'%(test_name,K))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_train = np.int64(data_train>0)\n",
    "filtered_data_test = np.int64(test_logcounts_share>0)\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_SuperCT.hkl'%(test_name,100))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_SuperCT.hkl'%(test_name,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alpha                     4910\n",
       "beta                      3715\n",
       "ductal                    1708\n",
       "acinar                    1362\n",
       "delta                      957\n",
       "gamma                      638\n",
       "endothelial                289\n",
       "activated_stellate         284\n",
       "quiescent_stellate         173\n",
       "mesenchymal                 80\n",
       "macrophage                  55\n",
       "PSC                         54\n",
       "unclassified endocrine      41\n",
       "co-expression               39\n",
       "mast                        32\n",
       "epsilon                     28\n",
       "schwann                     13\n",
       "t_cell                       7\n",
       "MHC class II                 5\n",
       "unclear                      4\n",
       "unclassified                 2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#人类胰腺=>人类脑细胞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "muraro_feature = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/muraro/muraro_feature.csv').iloc[:,1].values.tolist()\n",
    "baron_feature = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/baron/baron_feature.csv').iloc[:,1].values.tolist()\n",
    "xin_feature = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/xin/xin_feature.csv').iloc[:,1].values.tolist()\n",
    "segerstolpe_feature = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/segerstolpe/segerstolpe_feature.csv').iloc[:,1].values.tolist()\n",
    "darmanis_feature = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/darmanis/darmanis_feature.csv').iloc[:,1].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#darmanis专用\n",
    "test_name = 'darmanis'\n",
    "#找到共同基因集合\n",
    "muraro_baron = intersection(muraro_feature, baron_feature)\n",
    "muraro_baron_xin = intersection(muraro_baron, xin_feature)\n",
    "muraro_baron_xin_segerstolpe = intersection(muraro_baron_xin, segerstolpe_feature)\n",
    "muraro_baron_xin_segerstolpe_darmanis = intersection(muraro_baron_xin_segerstolpe, darmanis_feature)\n",
    "#找到这些相同基因位置\n",
    "muraro_feature_share = [muraro_feature.index(i) for i in muraro_baron_xin_segerstolpe_darmanis]\n",
    "baron_feature_share = [baron_feature.index(i) for i in muraro_baron_xin_segerstolpe_darmanis]\n",
    "xin_feature_share = [xin_feature.index(i) for i in muraro_baron_xin_segerstolpe_darmanis]\n",
    "segerstolpe_feature_share = [segerstolpe_feature.index(i) for i in muraro_baron_xin_segerstolpe_darmanis]\n",
    "darmanis_feature_share = [darmanis_feature.index(i) for i in muraro_baron_xin_segerstolpe_darmanis]\n",
    "#读取数据集\n",
    "muraro_logcounts = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/muraro/muraro_logcounts.csv').iloc[:,1:].values\n",
    "baron_logcounts = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/baron/baron_logcounts.csv').iloc[:,1:].values\n",
    "xin_logcounts = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/xin/xin_logcounts.csv').iloc[:,1:].values\n",
    "segerstolpe_logcounts = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/segerstolpe/segerstolpe_logcounts.csv').iloc[:,1:].values\n",
    "darmanis_logcounts = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/darmanis/darmanis_logcounts.csv').iloc[:,1:].values\n",
    "#所用数据集（未进行降维）\n",
    "muraro_logcounts_share = muraro_logcounts[muraro_feature_share, :]\n",
    "baron_logcounts_share = baron_logcounts[baron_feature_share, :]\n",
    "xin_logcounts_share = xin_logcounts[xin_feature_share, :]\n",
    "segerstolpe_logcounts_share = segerstolpe_logcounts[segerstolpe_feature_share, :]\n",
    "darmanis_logcounts_share = darmanis_logcounts[darmanis_feature_share,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15955"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(muraro_baron_xin_segerstolpe_darmanis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual'%test_name):\n",
    "    os.makedirs('/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual'%test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "muraro_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/muraro/muraro_label.csv').iloc[:,1].values.tolist()\n",
    "baron_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/baron/baron_label.csv').iloc[:,1].values.tolist()\n",
    "xin_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/xin/xin_label.csv').iloc[:,1].values.tolist()\n",
    "segerstolpe_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/segerstolpe/segerstolpe_label.csv').iloc[:,1].values.tolist()\n",
    "darmanis_label = pd.read_csv('/home/chenxiaoyang/program/scmap/Data/processed/darmanis/darmanis_label.csv').iloc[:,1].values.tolist()\n",
    "label_train = muraro_label+baron_label+xin_label+segerstolpe_label\n",
    "label_test = darmanis_label\n",
    "hkl.dump(label_train, '/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual/data_train_label.hkl'%test_name)\n",
    "hkl.dump(label_test, '/home/chenxiaoyang/program/scmap/Data/processed/%s/sorted_residual/data_test_label.hkl'%test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.hstack((muraro_logcounts_share,baron_logcounts_share, xin_logcounts_share, segerstolpe_logcounts_share))\n",
    "test_logcounts_share = darmanis_logcounts_share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "muraro_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(muraro_logcounts_share) - 1\n",
    "drop_feature = []\n",
    "X1 = []\n",
    "Y = []\n",
    "X2 = []\n",
    "feature_index_dict = []\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        Y.append(np.log(data_train_mean[i]+1))\n",
    "        X1.append(np.mean(muraro_logcounts_share[i,:]))\n",
    "        X2.append(np.log(drop_feature[i]))\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "X1 = np.array(X1).reshape(-1,1)\n",
    "X2 = np.array(X2).reshape(-1,1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X2,Y)\n",
    "predictions = lr.predict(X2)\n",
    "residuals = (Y-predictions).reshape(-1) + (Y - X1).reshape(-1)\n",
    "muraro_scores[feature_index_dict] = residuals\n",
    "baron_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(baron_logcounts_share) - 1\n",
    "drop_feature = []\n",
    "X1 = []\n",
    "Y = []\n",
    "X2 = []\n",
    "feature_index_dict = []\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        Y.append(np.log(data_train_mean[i]+1))\n",
    "        X1.append(np.mean(baron_logcounts_share[i,:]))\n",
    "        X2.append(np.log(drop_feature[i]))\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "X1 = np.array(X1).reshape(-1,1)\n",
    "X2 = np.array(X2).reshape(-1,1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X2,Y)\n",
    "predictions = lr.predict(X2)\n",
    "residuals = (Y-predictions).reshape(-1) + (Y - X1).reshape(-1)\n",
    "baron_scores[feature_index_dict] = residuals\n",
    "xin_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(xin_logcounts_share) - 1\n",
    "drop_feature = []\n",
    "X1 = []\n",
    "Y = []\n",
    "X2 = []\n",
    "feature_index_dict = []\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        Y.append(np.log(data_train_mean[i]+1))\n",
    "        X1.append(np.mean(xin_logcounts_share[i,:]))\n",
    "        X2.append(np.log(drop_feature[i]))\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "X1 = np.array(X1).reshape(-1,1)\n",
    "X2 = np.array(X2).reshape(-1,1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X2,Y)\n",
    "predictions = lr.predict(X2)\n",
    "residuals = (Y-predictions).reshape(-1) + (Y - X1).reshape(-1)\n",
    "xin_scores[feature_index_dict] = residuals\n",
    "segerstolpe_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(segerstolpe_logcounts_share) - 1\n",
    "drop_feature = []\n",
    "X1 = []\n",
    "Y = []\n",
    "X2 = []\n",
    "feature_index_dict = []\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        Y.append(np.log(data_train_mean[i]+1))\n",
    "        X1.append(np.mean(segerstolpe_logcounts_share[i,:]))\n",
    "        X2.append(np.log(drop_feature[i]))\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "X1 = np.array(X1).reshape(-1,1)\n",
    "X2 = np.array(X2).reshape(-1,1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X2,Y)\n",
    "predictions = lr.predict(X2)\n",
    "residuals = (Y-predictions).reshape(-1) + (Y - X1).reshape(-1)\n",
    "segerstolpe_scores[feature_index_dict] = residuals\n",
    "darmanis_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(darmanis_logcounts_share) - 1\n",
    "drop_feature = []\n",
    "X1 = []\n",
    "Y = []\n",
    "X2 = []\n",
    "feature_index_dict = []\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        Y.append(np.log(data_train_mean[i]+1))\n",
    "        X1.append(np.mean(darmanis_logcounts_share[i,:]))\n",
    "        X2.append(np.log(drop_feature[i]))\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "X1 = np.array(X1).reshape(-1,1)\n",
    "X2 = np.array(X2).reshape(-1,1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X2,Y)\n",
    "predictions = lr.predict(X2)\n",
    "residuals = (Y-predictions).reshape(-1) + (Y - X1).reshape(-1)\n",
    "darmanis_scores[feature_index_dict] = residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "K = 2600\n",
    "ind_muraro = np.argpartition(muraro_scores, -K)[-K:].tolist()\n",
    "ind_baron = np.argpartition(baron_scores, -K)[-K:].tolist()\n",
    "ind_xin = np.argpartition(xin_scores, -K)[-K:].tolist()\n",
    "ind_segerstolpe = np.argpartition(segerstolpe_scores, -K)[-K:].tolist()\n",
    "ind_darmanis = np.argpartition(darmanis_scores, -K)[-K:].tolist()\n",
    "\n",
    "ind_muraro_baron = intersection(ind_muraro, ind_baron)\n",
    "ind_muraro_baron_xin = intersection(ind_muraro_baron, ind_xin)\n",
    "ind_muraro_baron_xin_segerstolpe = intersection(ind_muraro_baron_xin, ind_segerstolpe)\n",
    "ind_muraro_baron_xin_segerstolpe_darmanis =  intersection(ind_muraro_baron_xin_segerstolpe,ind_darmanis)\n",
    "print(len(ind_muraro_baron_xin_segerstolpe_darmanis))\n",
    "filtered_data_train = data_train[ind_muraro_baron_xin_segerstolpe_darmanis, :]\n",
    "filtered_data_test = test_logcounts_share[ind_muraro_baron_xin_segerstolpe_darmanis, :]\n",
    "K = 100\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_myself.hkl'%(test_name,K))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_myself.hkl'%(test_name,K))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original = np.exp(baron_logcounts_share) - 1\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "data_train_var = np.var(data_original,1)\n",
    "data_train_mean_log = np.log(data_train_mean+1)\n",
    "data_train_var_log = np.log(data_train_var+1)\n",
    "ploy_reg = PolynomialFeatures(degree=2)\n",
    "x_ = ploy_reg.fit_transform(data_train_mean_log.reshape(-1,1))\n",
    "regr2 = linear_model.LinearRegression()\n",
    "regr2.fit(x_,data_train_var_log)\n",
    "y_pred = regr2.predict(x_)\n",
    "baron_scores = (data_train_var_log-y_pred).reshape(-1)\n",
    "data_original = np.exp(muraro_logcounts_share) - 1\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "data_train_var = np.var(data_original,1)\n",
    "data_train_mean_log = np.log(data_train_mean+1)\n",
    "data_train_var_log = np.log(data_train_var+1)\n",
    "ploy_reg = PolynomialFeatures(degree=2)\n",
    "x_ = ploy_reg.fit_transform(data_train_mean_log.reshape(-1,1))\n",
    "regr2 = linear_model.LinearRegression()\n",
    "regr2.fit(x_,data_train_var_log)\n",
    "y_pred = regr2.predict(x_)\n",
    "baron_scores = (data_train_var_log-y_pred).reshape(-1)\n",
    "data_original = np.exp(xin_logcounts_share) - 1\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "data_train_var = np.var(data_original,1)\n",
    "data_train_mean_log = np.log(data_train_mean+1)\n",
    "data_train_var_log = np.log(data_train_var+1)\n",
    "ploy_reg = PolynomialFeatures(degree=2)\n",
    "x_ = ploy_reg.fit_transform(data_train_mean_log.reshape(-1,1))\n",
    "regr2 = linear_model.LinearRegression()\n",
    "regr2.fit(x_,data_train_var_log)\n",
    "y_pred = regr2.predict(x_)\n",
    "xin_scores = (data_train_var_log-y_pred).reshape(-1)\n",
    "data_original = np.exp(segerstolpe_logcounts_share) - 1\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "data_train_var = np.var(data_original,1)\n",
    "data_train_mean_log = np.log(data_train_mean+1)\n",
    "data_train_var_log = np.log(data_train_var+1)\n",
    "ploy_reg = PolynomialFeatures(degree=2)\n",
    "x_ = ploy_reg.fit_transform(data_train_mean_log.reshape(-1,1))\n",
    "regr2 = linear_model.LinearRegression()\n",
    "regr2.fit(x_,data_train_var_log)\n",
    "y_pred = regr2.predict(x_)\n",
    "segerstolpe_scores = (data_train_var_log-y_pred).reshape(-1)\n",
    "data_original = np.exp(darmanis_logcounts_share) - 1\n",
    "data_train_mean = np.mean(data_original,1)\n",
    "data_train_var = np.var(data_original,1)\n",
    "data_train_mean_log = np.log(data_train_mean+1)\n",
    "data_train_var_log = np.log(data_train_var+1)\n",
    "ploy_reg = PolynomialFeatures(degree=2)\n",
    "x_ = ploy_reg.fit_transform(data_train_mean_log.reshape(-1,1))\n",
    "regr2 = linear_model.LinearRegression()\n",
    "regr2.fit(x_,data_train_var_log)\n",
    "y_pred = regr2.predict(x_)\n",
    "darmanis_scores = (data_train_var_log-y_pred).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "K = 3390\n",
    "ind_muraro = np.argpartition(muraro_scores, -K)[-K:].tolist()\n",
    "ind_baron = np.argpartition(baron_scores, -K)[-K:].tolist()\n",
    "ind_xin = np.argpartition(xin_scores, -K)[-K:].tolist()\n",
    "ind_segerstolpe = np.argpartition(segerstolpe_scores, -K)[-K:].tolist()\n",
    "ind_darmanis = np.argpartition(darmanis_scores, -K)[-K:].tolist()\n",
    "\n",
    "ind_muraro_baron = intersection(ind_muraro, ind_baron)\n",
    "ind_muraro_baron_xin = intersection(ind_muraro_baron, ind_xin)\n",
    "ind_muraro_baron_xin_segerstolpe = intersection(ind_muraro_baron_xin, ind_segerstolpe)\n",
    "ind_muraro_baron_xin_segerstolpe_darmanis =  intersection(ind_muraro_baron_xin_segerstolpe,ind_darmanis)\n",
    "print(len(ind_muraro_baron_xin_segerstolpe_darmanis))\n",
    "filtered_data_train = data_train[ind_muraro_baron_xin_segerstolpe_darmanis, :]\n",
    "filtered_data_test = test_logcounts_share[ind_muraro_baron_xin_segerstolpe_darmanis, :]\n",
    "K = 100\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_460147.hkl'%(test_name,K))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_460147.hkl'%(test_name,K))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train = min_max_scaler.fit_transform(data_train.T)\n",
    "X_test = min_max_scaler.transform(test_logcounts_share.T)\n",
    "data = np.hstack((muraro_logcounts_share, baron_logcounts_share, xin_logcounts_share, segerstolpe_logcounts_share))\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_ = min_max_scaler.fit_transform(data.T)\n",
    "X_train = min_max_scaler.transform(data_train.T)\n",
    "X_test = min_max_scaler.transform(test_logcounts_share.T)\n",
    "pca=PCA(n_components=100)\n",
    "pca.fit(X_)\n",
    "PCA_X_train = pca.transform(X_train)\n",
    "PCA_X_test = pca.transform(X_test)\n",
    "hkl.dump(PCA_X_train, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_PCA.hkl'%(test_name,100))\n",
    "hkl.dump(PCA_X_test, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_PCA.hkl'%(test_name,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15603,)\n",
      "(15451,)\n",
      "(15781,)\n",
      "(15802,)\n"
     ]
    }
   ],
   "source": [
    "#muraro scmap\n",
    "muraro_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(muraro_logcounts_share) - 1\n",
    "data_original_mean = np.mean(data_original,1)\n",
    "drop_feature = []\n",
    "\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "#print(drop_feature)\n",
    "feature_index_dict = []\n",
    "train_E = []\n",
    "train_D = []\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        train_E.append(data_original_mean[i])\n",
    "        train_D.append(drop_feature[i])\n",
    "train_E = np.array(train_E)\n",
    "train_D = np.array(train_D)\n",
    "train_E = np.log(train_E+1).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "train_D = np.log(100*train_D).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_E,train_D)\n",
    "predictions = lr.predict(train_E)\n",
    "residuals = (train_D-predictions).reshape(-1)\n",
    "#residuals = np.abs(residuals)\n",
    "muraro_scores[feature_index_dict] = residuals\n",
    "#xin scmap\n",
    "xin_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(xin_logcounts_share) - 1\n",
    "data_original_mean = np.mean(data_original,1)\n",
    "drop_feature = []\n",
    "\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "#print(drop_feature)\n",
    "feature_index_dict = []\n",
    "train_E = []\n",
    "train_D = []\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        train_E.append(data_original_mean[i])\n",
    "        train_D.append(drop_feature[i])\n",
    "train_E = np.array(train_E)\n",
    "train_D = np.array(train_D)\n",
    "train_E = np.log(train_E+1).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "train_D = np.log(100*train_D).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_E,train_D)\n",
    "predictions = lr.predict(train_E)\n",
    "residuals = (train_D-predictions).reshape(-1)\n",
    "#residuals = np.abs(residuals)\n",
    "xin_scores[feature_index_dict] = residuals\n",
    "print(residuals.shape)\n",
    "#baron scmap\n",
    "baron_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(baron_logcounts_share) - 1\n",
    "data_original_mean = np.mean(data_original,1)\n",
    "drop_feature = []\n",
    "\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "#print(drop_feature)\n",
    "feature_index_dict = []\n",
    "train_E = []\n",
    "train_D = []\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        train_E.append(data_original_mean[i])\n",
    "        train_D.append(drop_feature[i])\n",
    "train_E = np.array(train_E)\n",
    "train_D = np.array(train_D)\n",
    "train_E = np.log(train_E+1).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "train_D = np.log(100*train_D).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_E,train_D)\n",
    "predictions = lr.predict(train_E)\n",
    "residuals = (train_D-predictions).reshape(-1)\n",
    "#residuals = np.abs(residuals)\n",
    "baron_scores[feature_index_dict] = residuals\n",
    "print(residuals.shape)\n",
    "#segerstolpe scmap\n",
    "segerstolpe_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(segerstolpe_logcounts_share) - 1\n",
    "data_original_mean = np.mean(data_original,1)\n",
    "drop_feature = []\n",
    "\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "#print(drop_feature)\n",
    "feature_index_dict = []\n",
    "train_E = []\n",
    "train_D = []\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        train_E.append(data_original_mean[i])\n",
    "        train_D.append(drop_feature[i])\n",
    "train_E = np.array(train_E)\n",
    "train_D = np.array(train_D)\n",
    "train_E = np.log(train_E+1).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "train_D = np.log(100*train_D).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_E,train_D)\n",
    "predictions = lr.predict(train_E)\n",
    "residuals = (train_D-predictions).reshape(-1)\n",
    "#residuals = np.abs(residuals)\n",
    "segerstolpe_scores[feature_index_dict] = residuals\n",
    "print(residuals.shape)\n",
    "darmanis_scores = np.zeros(feature_number)-100\n",
    "data_original = np.exp(darmanis_logcounts_share) - 1\n",
    "data_original_mean = np.mean(data_original,1)\n",
    "drop_feature = []\n",
    "\n",
    "#寻找每个feature的0的数目（概率）\n",
    "for feature in data_original:\n",
    "    drop_feature.append(np.sum(feature==0)/feature.shape[0])\n",
    "#print(drop_feature)\n",
    "feature_index_dict = []\n",
    "train_E = []\n",
    "train_D = []\n",
    "for i in range(len(drop_feature)):\n",
    "    if (drop_feature[i]!=1 and drop_feature[i]!=0):\n",
    "        feature_index_dict.append(i)\n",
    "        train_E.append(data_original_mean[i])\n",
    "        train_D.append(drop_feature[i])\n",
    "train_E = np.array(train_E)\n",
    "train_D = np.array(train_D)\n",
    "train_E = np.log(train_E+1).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "train_D = np.log(100*train_D).reshape(-1,1)*math.log(2.7)/math.log(2)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_E,train_D)\n",
    "predictions = lr.predict(train_E)\n",
    "residuals = (train_D-predictions).reshape(-1)\n",
    "#residuals = np.abs(residuals)\n",
    "darmanis_scores[feature_index_dict] = residuals\n",
    "print(residuals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "K = 2630\n",
    "ind_muraro = np.argpartition(muraro_scores, -K)[-K:].tolist()\n",
    "ind_baron = np.argpartition(baron_scores, -K)[-K:].tolist()\n",
    "ind_xin = np.argpartition(xin_scores, -K)[-K:].tolist()\n",
    "ind_segerstolpe = np.argpartition(segerstolpe_scores, -K)[-K:].tolist()\n",
    "ind_darmanis = np.argpartition(darmanis_scores, -K)[-K:].tolist()\n",
    "\n",
    "ind_muraro_baron = intersection(ind_muraro, ind_baron)\n",
    "ind_muraro_baron_xin = intersection(ind_muraro_baron, ind_xin)\n",
    "ind_muraro_baron_xin_segerstolpe = intersection(ind_muraro_baron_xin, ind_segerstolpe)\n",
    "ind_muraro_baron_xin_segerstolpe_darmanis =  intersection(ind_muraro_baron_xin_segerstolpe,ind_darmanis)\n",
    "print(len(ind_muraro_baron_xin_segerstolpe_darmanis))\n",
    "filtered_data_train = data_train[ind_muraro_baron_xin_segerstolpe_darmanis, :]\n",
    "filtered_data_test = test_logcounts_share[ind_muraro_baron_xin_segerstolpe_darmanis, :]\n",
    "K = 100\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_scmap.hkl'%(test_name,K))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_scmap.hkl'%(test_name,K))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_train = np.int64(data_train>0)\n",
    "filtered_data_test = np.int64(test_logcounts_share>0)\n",
    "hkl.dump(filtered_data_train.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_train_%d_sample_SuperCT.hkl'%(test_name,100))\n",
    "hkl.dump(filtered_data_test.T, '/home/chenxiaoyang/program/scmap/Data/processed/%s/data_test_%d_sample_SuperCT.hkl'%(test_name,100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
